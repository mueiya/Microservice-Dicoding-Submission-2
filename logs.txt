* 
* ==> Audit <==
* |---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 09:09 WIB | 26 Oct 23 09:15 WIB |
| ip      |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:10 WIB | 26 Oct 23 11:10 WIB |
| ip      |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:11 WIB | 26 Oct 23 11:11 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:31 WIB | 26 Oct 23 11:31 WIB |
| delete  |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:31 WIB | 26 Oct 23 11:31 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:34 WIB | 26 Oct 23 11:35 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:49 WIB | 26 Oct 23 11:49 WIB |
| delete  |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 11:49 WIB | 26 Oct 23 11:49 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:12 WIB | 26 Oct 23 12:13 WIB |
| delete  |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:18 WIB | 26 Oct 23 12:19 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:21 WIB | 26 Oct 23 12:22 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:41 WIB | 26 Oct 23 12:41 WIB |
| delete  |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:41 WIB | 26 Oct 23 12:41 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 12:42 WIB | 26 Oct 23 12:43 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 13:02 WIB | 26 Oct 23 13:03 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 13:03 WIB | 26 Oct 23 13:03 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 26 Oct 23 15:36 WIB | 26 Oct 23 15:36 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:17 WIB |                     |
| start   |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:20 WIB | 05 Nov 23 20:21 WIB |
| ip      |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:29 WIB | 05 Nov 23 20:29 WIB |
| stop    |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:33 WIB | 05 Nov 23 20:33 WIB |
| start   |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:37 WIB | 05 Nov 23 20:38 WIB |
| ip      |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:40 WIB | 05 Nov 23 20:40 WIB |
| tunnel  |                                | minikube | mueiya | v1.31.2 | 05 Nov 23 20:46 WIB | 05 Nov 23 20:50 WIB |
| service | karsajobs-ui-service --url     | minikube | mueiya | v1.31.2 | 05 Nov 23 20:59 WIB |                     |
| service | karsajobs-ui-service -n        | minikube | mueiya | v1.31.2 | 05 Nov 23 21:00 WIB |                     |
|         | karsajobs-app --url            |          |        |         |                     |                     |
| service | karsajobs-ui-service -n        | minikube | mueiya | v1.31.2 | 05 Nov 23 21:10 WIB |                     |
|         | karsajobs-app --url            |          |        |         |                     |                     |
| service | karsajobs-ui -n karsajobs-app  | minikube | mueiya | v1.31.2 | 05 Nov 23 21:11 WIB |                     |
|         | --url                          |          |        |         |                     |                     |
| service | karsajobs-ui -n karsajobs-app  | minikube | mueiya | v1.31.2 | 05 Nov 23 21:24 WIB |                     |
|         | --url                          |          |        |         |                     |                     |
| service | karsajobs-ui -n karsajobs-app  | minikube | mueiya | v1.31.2 | 05 Nov 23 21:29 WIB |                     |
|         | --url                          |          |        |         |                     |                     |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/05 20:37:28
Running on machine: Sesame002
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1105 20:37:28.839305    9137 out.go:296] Setting OutFile to fd 1 ...
I1105 20:37:28.839586    9137 out.go:348] isatty.IsTerminal(1) = true
I1105 20:37:28.839591    9137 out.go:309] Setting ErrFile to fd 2...
I1105 20:37:28.839596    9137 out.go:348] isatty.IsTerminal(2) = true
I1105 20:37:28.840108    9137 root.go:338] Updating PATH: /home/mueiya/.minikube/bin
I1105 20:37:28.840844    9137 out.go:303] Setting JSON to false
I1105 20:37:28.846365    9137 start.go:128] hostinfo: {"hostname":"Sesame002","uptime":1532,"bootTime":1699189917,"procs":68,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.90.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"4a14bd86-e31b-4dd3-98d2-21d10e85724a"}
I1105 20:37:28.846718    9137 start.go:138] virtualization:  guest
I1105 20:37:28.907142    9137 out.go:177] üòÑ  minikube v1.31.2 on Ubuntu 22.04 (amd64)
I1105 20:37:28.917193    9137 notify.go:220] Checking for updates...
I1105 20:37:28.917542    9137 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1105 20:37:28.924431    9137 driver.go:373] Setting default libvirt URI to qemu:///system
I1105 20:37:29.061661    9137 docker.go:121] docker version: linux-24.0.2:Docker Desktop
I1105 20:37:29.061879    9137 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1105 20:37:30.097405    9137 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.035456049s)
I1105 20:37:30.107180    9137 info.go:266] docker info: {ID:9cdbd56d-8aa3-41ee-9138-798452f85c1e Containers:4 ContainersRunning:3 ContainersPaused:0 ContainersStopped:1 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:87 SystemTime:2023-11-05 13:37:30.057743821 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:7238074368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.0] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.16.1]] Warnings:<nil>}}
I1105 20:37:30.107604    9137 docker.go:294] overlay module found
I1105 20:37:30.112923    9137 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1105 20:37:30.119867    9137 start.go:298] selected driver: docker
I1105 20:37:30.119879    9137 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mueiya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1105 20:37:30.120017    9137 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1105 20:37:30.124002    9137 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1105 20:37:30.493317    9137 info.go:266] docker info: {ID:9cdbd56d-8aa3-41ee-9138-798452f85c1e Containers:4 ContainersRunning:3 ContainersPaused:0 ContainersStopped:1 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:87 SystemTime:2023-11-05 13:37:30.470873712 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:7238074368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.0] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.16.1]] Warnings:<nil>}}
I1105 20:37:30.494384    9137 cni.go:84] Creating CNI manager for ""
I1105 20:37:30.494488    9137 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1105 20:37:30.494624    9137 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mueiya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1105 20:37:30.499488    9137 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1105 20:37:30.502606    9137 cache.go:122] Beginning downloading kic base image for docker with docker
I1105 20:37:30.504458    9137 out.go:177] üöú  Pulling base image ...
I1105 20:37:30.506617    9137 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1105 20:37:30.506844    9137 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1105 20:37:30.506928    9137 preload.go:148] Found local preload: /home/mueiya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1105 20:37:30.507032    9137 cache.go:57] Caching tarball of preloaded images
I1105 20:37:30.508163    9137 preload.go:174] Found /home/mueiya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1105 20:37:30.508257    9137 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1105 20:37:30.508532    9137 profile.go:148] Saving config to /home/mueiya/.minikube/profiles/minikube/config.json ...
I1105 20:37:30.605082    9137 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I1105 20:37:30.605221    9137 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I1105 20:37:30.605376    9137 cache.go:195] Successfully downloaded all kic artifacts
I1105 20:37:30.605591    9137 start.go:365] acquiring machines lock for minikube: {Name:mke4503d90b8ae4a3cc13477da2581e12c53c6c4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1105 20:37:30.605663    9137 start.go:369] acquired machines lock for "minikube" in 46.63¬µs
I1105 20:37:30.606637    9137 start.go:96] Skipping create...Using existing machine configuration
I1105 20:37:30.606843    9137 fix.go:54] fixHost starting: 
I1105 20:37:30.607161    9137 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1105 20:37:30.707255    9137 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1105 20:37:30.707274    9137 fix.go:128] unexpected machine state, will restart: <nil>
I1105 20:37:30.710512    9137 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1105 20:37:30.712505    9137 cli_runner.go:164] Run: docker start minikube
I1105 20:37:31.453252    9137 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1105 20:37:31.648132    9137 kic.go:426] container "minikube" state is running.
I1105 20:37:31.650415    9137 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1105 20:37:31.852472    9137 profile.go:148] Saving config to /home/mueiya/.minikube/profiles/minikube/config.json ...
I1105 20:37:31.852742    9137 machine.go:88] provisioning docker machine ...
I1105 20:37:31.853290    9137 ubuntu.go:169] provisioning hostname "minikube"
I1105 20:37:31.853832    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:32.080670    9137 main.go:141] libmachine: Using SSH client type: native
I1105 20:37:32.083527    9137 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 61381 <nil> <nil>}
I1105 20:37:32.083540    9137 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1105 20:37:32.091030    9137 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1105 20:37:35.282489    9137 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1105 20:37:35.283914    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:35.413876    9137 main.go:141] libmachine: Using SSH client type: native
I1105 20:37:35.414292    9137 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 61381 <nil> <nil>}
I1105 20:37:35.414305    9137 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1105 20:37:35.568653    9137 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1105 20:37:35.568990    9137 ubuntu.go:175] set auth options {CertDir:/home/mueiya/.minikube CaCertPath:/home/mueiya/.minikube/certs/ca.pem CaPrivateKeyPath:/home/mueiya/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/mueiya/.minikube/machines/server.pem ServerKeyPath:/home/mueiya/.minikube/machines/server-key.pem ClientKeyPath:/home/mueiya/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/mueiya/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/mueiya/.minikube}
I1105 20:37:35.569012    9137 ubuntu.go:177] setting up certificates
I1105 20:37:35.569020    9137 provision.go:83] configureAuth start
I1105 20:37:35.569092    9137 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1105 20:37:35.677875    9137 provision.go:138] copyHostCerts
I1105 20:37:35.678605    9137 exec_runner.go:144] found /home/mueiya/.minikube/ca.pem, removing ...
I1105 20:37:35.678705    9137 exec_runner.go:203] rm: /home/mueiya/.minikube/ca.pem
I1105 20:37:35.679318    9137 exec_runner.go:151] cp: /home/mueiya/.minikube/certs/ca.pem --> /home/mueiya/.minikube/ca.pem (1078 bytes)
I1105 20:37:35.679777    9137 exec_runner.go:144] found /home/mueiya/.minikube/cert.pem, removing ...
I1105 20:37:35.679786    9137 exec_runner.go:203] rm: /home/mueiya/.minikube/cert.pem
I1105 20:37:35.679836    9137 exec_runner.go:151] cp: /home/mueiya/.minikube/certs/cert.pem --> /home/mueiya/.minikube/cert.pem (1123 bytes)
I1105 20:37:35.680267    9137 exec_runner.go:144] found /home/mueiya/.minikube/key.pem, removing ...
I1105 20:37:35.680277    9137 exec_runner.go:203] rm: /home/mueiya/.minikube/key.pem
I1105 20:37:35.680335    9137 exec_runner.go:151] cp: /home/mueiya/.minikube/certs/key.pem --> /home/mueiya/.minikube/key.pem (1679 bytes)
I1105 20:37:35.680601    9137 provision.go:112] generating server cert: /home/mueiya/.minikube/machines/server.pem ca-key=/home/mueiya/.minikube/certs/ca.pem private-key=/home/mueiya/.minikube/certs/ca-key.pem org=mueiya.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1105 20:37:35.993080    9137 provision.go:172] copyRemoteCerts
I1105 20:37:35.993384    9137 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1105 20:37:35.993498    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:36.103836    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:37:36.242432    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1105 20:37:36.338516    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1105 20:37:36.389128    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1105 20:37:36.437308    9137 provision.go:86] duration metric: configureAuth took 867.83187ms
I1105 20:37:36.437333    9137 ubuntu.go:193] setting minikube options for container-runtime
I1105 20:37:36.437529    9137 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1105 20:37:36.437647    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:36.540994    9137 main.go:141] libmachine: Using SSH client type: native
I1105 20:37:36.541418    9137 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 61381 <nil> <nil>}
I1105 20:37:36.541426    9137 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1105 20:37:36.718215    9137 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1105 20:37:36.718229    9137 ubuntu.go:71] root file system type: overlay
I1105 20:37:36.718357    9137 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1105 20:37:36.718431    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:36.812068    9137 main.go:141] libmachine: Using SSH client type: native
I1105 20:37:36.812438    9137 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 61381 <nil> <nil>}
I1105 20:37:36.812491    9137 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1105 20:37:36.984024    9137 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1105 20:37:36.984194    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:37.077206    9137 main.go:141] libmachine: Using SSH client type: native
I1105 20:37:37.077615    9137 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 61381 <nil> <nil>}
I1105 20:37:37.077629    9137 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1105 20:37:37.223777    9137 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1105 20:37:37.223790    9137 machine.go:91] provisioned docker machine in 5.371039706s
I1105 20:37:37.223966    9137 start.go:300] post-start starting for "minikube" (driver="docker")
I1105 20:37:37.224117    9137 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1105 20:37:37.224194    9137 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1105 20:37:37.224247    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:37.312925    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:37:37.420394    9137 ssh_runner.go:195] Run: cat /etc/os-release
I1105 20:37:37.424851    9137 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1105 20:37:37.424875    9137 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1105 20:37:37.424883    9137 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1105 20:37:37.424889    9137 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1105 20:37:37.425009    9137 filesync.go:126] Scanning /home/mueiya/.minikube/addons for local assets ...
I1105 20:37:37.425255    9137 filesync.go:126] Scanning /home/mueiya/.minikube/files for local assets ...
I1105 20:37:37.425384    9137 start.go:303] post-start completed in 201.408442ms
I1105 20:37:37.425475    9137 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1105 20:37:37.425522    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:37.534313    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:37:37.630521    9137 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1105 20:37:37.637127    9137 fix.go:56] fixHost completed within 7.030301007s
I1105 20:37:37.637139    9137 start.go:83] releasing machines lock for "minikube", held for 7.03147019s
I1105 20:37:37.637376    9137 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1105 20:37:37.734068    9137 ssh_runner.go:195] Run: cat /version.json
I1105 20:37:37.734118    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:37.734307    9137 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1105 20:37:37.734591    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:37:37.852312    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:37:37.901996    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:37:37.987035    9137 ssh_runner.go:195] Run: systemctl --version
I1105 20:37:38.396771    9137 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1105 20:37:38.403254    9137 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1105 20:37:38.438741    9137 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1105 20:37:38.438821    9137 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1105 20:37:38.455587    9137 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1105 20:37:38.455602    9137 start.go:466] detecting cgroup driver to use...
I1105 20:37:38.455705    9137 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1105 20:37:38.457688    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1105 20:37:38.489636    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1105 20:37:38.507598    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1105 20:37:38.525440    9137 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1105 20:37:38.525512    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1105 20:37:38.548208    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1105 20:37:38.569031    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1105 20:37:38.587908    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1105 20:37:38.609450    9137 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1105 20:37:38.628150    9137 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1105 20:37:38.645607    9137 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1105 20:37:38.663147    9137 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1105 20:37:38.678943    9137 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1105 20:37:38.816492    9137 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1105 20:37:38.921845    9137 start.go:466] detecting cgroup driver to use...
I1105 20:37:38.921882    9137 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1105 20:37:38.922126    9137 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1105 20:37:38.949181    9137 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1105 20:37:38.949244    9137 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1105 20:37:38.970991    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1105 20:37:39.003771    9137 ssh_runner.go:195] Run: which cri-dockerd
I1105 20:37:39.010528    9137 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1105 20:37:39.031253    9137 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1105 20:37:39.084907    9137 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1105 20:37:39.244410    9137 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1105 20:37:39.381755    9137 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1105 20:37:39.381779    9137 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1105 20:37:39.410595    9137 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1105 20:37:39.556315    9137 ssh_runner.go:195] Run: sudo systemctl restart docker
I1105 20:37:39.939758    9137 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1105 20:37:40.064445    9137 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1105 20:37:40.203357    9137 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1105 20:37:40.326915    9137 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1105 20:37:40.446324    9137 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1105 20:37:40.467731    9137 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1105 20:37:40.581586    9137 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1105 20:37:41.030726    9137 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1105 20:37:41.030792    9137 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1105 20:37:41.044598    9137 start.go:534] Will wait 60s for crictl version
I1105 20:37:41.044671    9137 ssh_runner.go:195] Run: which crictl
I1105 20:37:41.050303    9137 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1105 20:37:41.471147    9137 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1105 20:37:41.471221    9137 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1105 20:37:41.539846    9137 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1105 20:37:41.601685    9137 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1105 20:37:41.602038    9137 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1105 20:37:41.717596    9137 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1105 20:37:41.722300    9137 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1105 20:37:41.740921    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1105 20:37:41.860264    9137 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1105 20:37:41.860373    9137 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1105 20:37:41.889333    9137 docker.go:636] Got preloaded images: -- stdout --
ghcr.io/mueiya/karsajobs:latest
mongo:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1105 20:37:41.891438    9137 docker.go:566] Images already preloaded, skipping extraction
I1105 20:37:41.891728    9137 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1105 20:37:41.912919    9137 docker.go:636] Got preloaded images: -- stdout --
ghcr.io/mueiya/karsajobs:latest
mongo:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1105 20:37:41.913044    9137 cache_images.go:84] Images are preloaded, skipping loading
I1105 20:37:41.913286    9137 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1105 20:37:42.487602    9137 cni.go:84] Creating CNI manager for ""
I1105 20:37:42.487618    9137 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1105 20:37:42.487913    9137 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1105 20:37:42.488327    9137 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1105 20:37:42.489142    9137 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1105 20:37:42.489947    9137 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1105 20:37:42.493062    9137 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1105 20:37:42.514963    9137 binaries.go:44] Found k8s binaries, skipping transfer
I1105 20:37:42.515023    9137 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1105 20:37:42.538211    9137 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1105 20:37:42.569215    9137 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1105 20:37:42.601698    9137 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1105 20:37:42.639982    9137 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1105 20:37:42.644602    9137 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1105 20:37:42.670829    9137 certs.go:56] Setting up /home/mueiya/.minikube/profiles/minikube for IP: 192.168.49.2
I1105 20:37:42.671255    9137 certs.go:190] acquiring lock for shared ca certs: {Name:mkec3fb2fd2b059ccbb7325877e6e1e6c9a5c96c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1105 20:37:42.671786    9137 certs.go:199] skipping minikubeCA CA generation: /home/mueiya/.minikube/ca.key
I1105 20:37:42.672059    9137 certs.go:199] skipping proxyClientCA CA generation: /home/mueiya/.minikube/proxy-client-ca.key
I1105 20:37:42.672263    9137 certs.go:315] skipping minikube-user signed cert generation: /home/mueiya/.minikube/profiles/minikube/client.key
I1105 20:37:42.672303    9137 certs.go:315] skipping minikube signed cert generation: /home/mueiya/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1105 20:37:42.672499    9137 certs.go:315] skipping aggregator signed cert generation: /home/mueiya/.minikube/profiles/minikube/proxy-client.key
I1105 20:37:42.673074    9137 certs.go:437] found cert: /home/mueiya/.minikube/certs/home/mueiya/.minikube/certs/ca-key.pem (1679 bytes)
I1105 20:37:42.673116    9137 certs.go:437] found cert: /home/mueiya/.minikube/certs/home/mueiya/.minikube/certs/ca.pem (1078 bytes)
I1105 20:37:42.673136    9137 certs.go:437] found cert: /home/mueiya/.minikube/certs/home/mueiya/.minikube/certs/cert.pem (1123 bytes)
I1105 20:37:42.673152    9137 certs.go:437] found cert: /home/mueiya/.minikube/certs/home/mueiya/.minikube/certs/key.pem (1679 bytes)
I1105 20:37:42.674913    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1105 20:37:42.752697    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1105 20:37:42.819596    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1105 20:37:42.882616    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1105 20:37:42.928999    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1105 20:37:43.003472    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1105 20:37:43.052710    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1105 20:37:43.097459    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1105 20:37:43.160266    9137 ssh_runner.go:362] scp /home/mueiya/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1105 20:37:43.218865    9137 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1105 20:37:43.271058    9137 ssh_runner.go:195] Run: openssl version
I1105 20:37:43.283322    9137 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1105 20:37:43.303707    9137 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1105 20:37:43.310437    9137 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 26 02:14 /usr/share/ca-certificates/minikubeCA.pem
I1105 20:37:43.310491    9137 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1105 20:37:43.319330    9137 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1105 20:37:43.334750    9137 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1105 20:37:43.341260    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1105 20:37:43.350149    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1105 20:37:43.360162    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1105 20:37:43.382345    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1105 20:37:43.407434    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1105 20:37:43.419944    9137 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1105 20:37:43.429884    9137 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mueiya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1105 20:37:43.430015    9137 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1105 20:37:43.453895    9137 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1105 20:37:43.471722    9137 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1105 20:37:43.472303    9137 kubeadm.go:636] restartCluster start
I1105 20:37:43.472432    9137 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1105 20:37:43.487871    9137 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1105 20:37:43.487941    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1105 20:37:43.581385    9137 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/mueiya/.kube/config
I1105 20:37:43.581503    9137 kubeconfig.go:146] "minikube" context is missing from /home/mueiya/.kube/config - will repair!
I1105 20:37:43.583054    9137 lock.go:35] WriteFile acquiring /home/mueiya/.kube/config: {Name:mk61eec32da545b7d04e8f1bb6d97dd5965e7b39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1105 20:37:43.592888    9137 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1105 20:37:43.610123    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:43.610264    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:43.633269    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:43.633283    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:43.633342    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:43.648506    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:44.149409    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:44.149491    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:44.165177    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:44.649150    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:44.649244    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:44.666766    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:45.149474    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:45.149542    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:45.167637    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:45.648761    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:45.648827    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:45.669504    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:46.149188    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:46.149247    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:46.165438    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:46.649506    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:46.649585    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:46.667758    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:47.149597    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:47.149663    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:47.164832    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:47.648756    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:47.648975    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:47.668355    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:48.149036    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:48.149133    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:48.166788    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:48.649259    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:48.649326    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:48.675451    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:49.149017    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:49.149078    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:49.166395    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:49.649716    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:49.649786    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:49.669300    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:50.148776    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:50.148851    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:50.167426    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:50.648851    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:50.648925    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:50.670267    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:51.148986    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:51.149069    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:51.173730    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:51.649604    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:51.649692    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:51.668641    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:52.149583    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:52.149658    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:52.167021    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:52.649031    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:52.649093    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:52.668839    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:53.148840    9137 api_server.go:166] Checking apiserver status ...
I1105 20:37:53.148910    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1105 20:37:53.172371    9137 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1105 20:37:53.648789    9137 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1105 20:37:53.648880    9137 kubeadm.go:1128] stopping kube-system containers ...
I1105 20:37:53.648964    9137 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1105 20:37:53.683963    9137 docker.go:462] Stopping containers: [b6048cb4ac72 6a51ba832c7b 2e205cd7fc79 50f32578fa90 cac507624666 45cb9a138182 61e6b6cfc33f 310f2eae0f35 09d86773b284 b38dbc622c93 f2081a31d0a6 ca393e85decb 52d69bc482e3 51ceb1875287 8ef75b356b0b cef313ba9c0b 6af9c0567161 9b6de744c44d 58794ae67015 b7803a0b53f2 ad46a28c716c 5ec6a1f1fb84 31a48b915d47 238674bc497e a2a12ab837d2 fdcaea70312f e2494c20c49b]
I1105 20:37:53.684041    9137 ssh_runner.go:195] Run: docker stop b6048cb4ac72 6a51ba832c7b 2e205cd7fc79 50f32578fa90 cac507624666 45cb9a138182 61e6b6cfc33f 310f2eae0f35 09d86773b284 b38dbc622c93 f2081a31d0a6 ca393e85decb 52d69bc482e3 51ceb1875287 8ef75b356b0b cef313ba9c0b 6af9c0567161 9b6de744c44d 58794ae67015 b7803a0b53f2 ad46a28c716c 5ec6a1f1fb84 31a48b915d47 238674bc497e a2a12ab837d2 fdcaea70312f e2494c20c49b
I1105 20:37:53.716183    9137 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1105 20:37:53.739191    9137 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1105 20:37:53.757368    9137 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Oct 26 05:43 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Nov  5 13:20 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Oct 26 05:43 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Nov  5 13:20 /etc/kubernetes/scheduler.conf

I1105 20:37:53.757425    9137 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1105 20:37:53.775301    9137 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1105 20:37:53.795792    9137 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1105 20:37:53.813024    9137 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1105 20:37:53.813391    9137 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1105 20:37:53.831674    9137 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1105 20:37:53.850163    9137 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1105 20:37:53.850219    9137 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1105 20:37:53.865962    9137 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1105 20:37:53.882935    9137 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1105 20:37:53.882949    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:37:54.061182    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:37:55.564563    9137 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.503745377s)
I1105 20:37:55.564578    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:37:55.753497    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:37:55.822534    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:37:55.887484    9137 api_server.go:52] waiting for apiserver process to appear ...
I1105 20:37:55.887559    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:55.906926    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:56.428544    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:56.928966    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:57.429150    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:57.928470    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:37:58.013074    9137 api_server.go:72] duration metric: took 2.125691027s to wait for apiserver process to appear ...
I1105 20:37:58.013088    9137 api_server.go:88] waiting for apiserver healthz status ...
I1105 20:37:58.013103    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:37:58.015122    9137 api_server.go:269] stopped: https://127.0.0.1:61385/healthz: Get "https://127.0.0.1:61385/healthz": EOF
I1105 20:37:58.015155    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:37:58.025113    9137 api_server.go:269] stopped: https://127.0.0.1:61385/healthz: Get "https://127.0.0.1:61385/healthz": EOF
I1105 20:37:58.526243    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:37:58.527873    9137 api_server.go:269] stopped: https://127.0.0.1:61385/healthz: Get "https://127.0.0.1:61385/healthz": EOF
I1105 20:37:59.025563    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:37:59.026983    9137 api_server.go:269] stopped: https://127.0.0.1:61385/healthz: Get "https://127.0.0.1:61385/healthz": EOF
I1105 20:37:59.528588    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:03.928475    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1105 20:38:03.928492    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1105 20:38:03.928503    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:04.225555    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1105 20:38:04.225571    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1105 20:38:04.225582    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:04.351427    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1105 20:38:04.351444    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1105 20:38:04.528758    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:04.776565    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:04.776585    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:05.025890    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:05.133393    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:05.133417    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:05.530975    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:05.613113    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:05.613139    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:06.040094    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:06.120757    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:06.120780    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:06.526087    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:06.611599    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:06.611620    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:07.028144    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:07.137083    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:07.137101    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:07.525841    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:07.558901    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:07.558921    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:08.052893    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:08.104677    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:08.104696    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:08.526239    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:08.569963    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:08.569987    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:09.026589    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:09.104612    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1105 20:38:09.104632    9137 api_server.go:103] status: https://127.0.0.1:61385/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1105 20:38:09.532754    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:09.604239    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 200:
ok
I1105 20:38:09.645822    9137 api_server.go:141] control plane version: v1.27.4
I1105 20:38:09.645844    9137 api_server.go:131] duration metric: took 11.632751384s to wait for apiserver health ...
I1105 20:38:09.645891    9137 cni.go:84] Creating CNI manager for ""
I1105 20:38:09.645934    9137 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1105 20:38:09.656917    9137 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1105 20:38:09.666998    9137 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1105 20:38:09.913692    9137 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1105 20:38:11.629497    9137 system_pods.go:43] waiting for kube-system pods to appear ...
I1105 20:38:11.911598    9137 system_pods.go:59] 7 kube-system pods found
I1105 20:38:11.911720    9137 system_pods.go:61] "coredns-5d78c9869d-6hfcj" [e30627ab-5476-44db-8aa8-7075bfbd427c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1105 20:38:11.911732    9137 system_pods.go:61] "etcd-minikube" [90be9883-ff80-4216-8e36-4d2078bfd3ae] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1105 20:38:11.911740    9137 system_pods.go:61] "kube-apiserver-minikube" [f161a033-2dea-44ca-92e0-27b1507394a4] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1105 20:38:11.911748    9137 system_pods.go:61] "kube-controller-manager-minikube" [9b6e8269-8919-404d-9446-85f3ab6b541a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1105 20:38:11.911753    9137 system_pods.go:61] "kube-proxy-mm465" [e814cb00-58e1-4a03-90f1-338741e6b5fa] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1105 20:38:11.911759    9137 system_pods.go:61] "kube-scheduler-minikube" [8296dd7e-1d5e-4bbb-8483-46ce425b45f9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1105 20:38:11.911763    9137 system_pods.go:61] "storage-provisioner" [a715d5b3-5483-4d9e-9636-35959329f363] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1105 20:38:11.911769    9137 system_pods.go:74] duration metric: took 282.255812ms to wait for pod list to return data ...
I1105 20:38:11.911837    9137 node_conditions.go:102] verifying NodePressure condition ...
I1105 20:38:12.065783    9137 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1105 20:38:12.065878    9137 node_conditions.go:123] node cpu capacity is 4
I1105 20:38:12.066788    9137 node_conditions.go:105] duration metric: took 154.93461ms to run NodePressure ...
I1105 20:38:12.066841    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1105 20:38:14.240313    9137 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.17344435s)
I1105 20:38:14.240342    9137 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1105 20:38:14.308087    9137 ops.go:34] apiserver oom_adj: -16
I1105 20:38:14.308100    9137 kubeadm.go:640] restartCluster took 30.83617579s
I1105 20:38:14.308108    9137 kubeadm.go:406] StartCluster complete in 30.878669692s
I1105 20:38:14.308293    9137 settings.go:142] acquiring lock: {Name:mkaecf411b32733fd2d8bdb8368297e0b1fa1cc7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1105 20:38:14.308413    9137 settings.go:150] Updating kubeconfig:  /home/mueiya/.kube/config
I1105 20:38:14.309039    9137 lock.go:35] WriteFile acquiring /home/mueiya/.kube/config: {Name:mk61eec32da545b7d04e8f1bb6d97dd5965e7b39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1105 20:38:14.314791    9137 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1105 20:38:14.315656    9137 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1105 20:38:14.318729    9137 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1105 20:38:14.319237    9137 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1105 20:38:14.319504    9137 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1105 20:38:14.319518    9137 addons.go:240] addon storage-provisioner should already be in state true
I1105 20:38:14.320409    9137 host.go:66] Checking if "minikube" exists ...
I1105 20:38:14.320831    9137 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1105 20:38:14.321022    9137 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1105 20:38:14.321131    9137 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1105 20:38:14.322304    9137 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1105 20:38:14.347362    9137 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1105 20:38:14.347453    9137 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1105 20:38:14.385705    9137 out.go:177] üîé  Verifying Kubernetes components...
I1105 20:38:14.392035    9137 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1105 20:38:14.852599    9137 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1105 20:38:14.852617    9137 addons.go:240] addon default-storageclass should already be in state true
I1105 20:38:14.852646    9137 host.go:66] Checking if "minikube" exists ...
I1105 20:38:14.853087    9137 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1105 20:38:14.955096    9137 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1105 20:38:14.987391    9137 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1105 20:38:14.987404    9137 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1105 20:38:14.987494    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:38:15.253666    9137 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1105 20:38:15.253687    9137 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1105 20:38:15.253778    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1105 20:38:15.313997    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:38:15.581134    9137 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61381 SSHKeyPath:/home/mueiya/.minikube/machines/minikube/id_rsa Username:docker}
I1105 20:38:15.752190    9137 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1105 20:38:16.421332    9137 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1105 20:38:17.606095    9137 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (3.2904116s)
I1105 20:38:17.606284    9137 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1105 20:38:17.606308    9137 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (3.214258857s)
I1105 20:38:17.606390    9137 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1105 20:38:18.810452    9137 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube: (1.20402836s)
I1105 20:38:18.810490    9137 api_server.go:52] waiting for apiserver process to appear ...
I1105 20:38:18.810551    9137 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1105 20:38:25.734567    9137 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.979669565s)
I1105 20:38:25.734611    9137 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (9.310588504s)
I1105 20:38:25.739779    9137 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1105 20:38:25.734820    9137 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (6.921575186s)
I1105 20:38:25.742279    9137 addons.go:502] enable addons completed in 11.424971178s: enabled=[storage-provisioner default-storageclass]
I1105 20:38:25.742371    9137 api_server.go:72] duration metric: took 11.392207456s to wait for apiserver process to appear ...
I1105 20:38:25.742379    9137 api_server.go:88] waiting for apiserver healthz status ...
I1105 20:38:25.742394    9137 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61385/healthz ...
I1105 20:38:25.758160    9137 api_server.go:279] https://127.0.0.1:61385/healthz returned 200:
ok
I1105 20:38:25.760171    9137 api_server.go:141] control plane version: v1.27.4
I1105 20:38:25.760189    9137 api_server.go:131] duration metric: took 17.804755ms to wait for apiserver health ...
I1105 20:38:25.760196    9137 system_pods.go:43] waiting for kube-system pods to appear ...
I1105 20:38:25.776613    9137 system_pods.go:59] 7 kube-system pods found
I1105 20:38:25.776660    9137 system_pods.go:61] "coredns-5d78c9869d-6hfcj" [e30627ab-5476-44db-8aa8-7075bfbd427c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1105 20:38:25.776669    9137 system_pods.go:61] "etcd-minikube" [90be9883-ff80-4216-8e36-4d2078bfd3ae] Running
I1105 20:38:25.776673    9137 system_pods.go:61] "kube-apiserver-minikube" [f161a033-2dea-44ca-92e0-27b1507394a4] Running
I1105 20:38:25.776677    9137 system_pods.go:61] "kube-controller-manager-minikube" [9b6e8269-8919-404d-9446-85f3ab6b541a] Running
I1105 20:38:25.776681    9137 system_pods.go:61] "kube-proxy-mm465" [e814cb00-58e1-4a03-90f1-338741e6b5fa] Running
I1105 20:38:25.776684    9137 system_pods.go:61] "kube-scheduler-minikube" [8296dd7e-1d5e-4bbb-8483-46ce425b45f9] Running
I1105 20:38:25.776689    9137 system_pods.go:61] "storage-provisioner" [a715d5b3-5483-4d9e-9636-35959329f363] Running
I1105 20:38:25.776695    9137 system_pods.go:74] duration metric: took 16.495005ms to wait for pod list to return data ...
I1105 20:38:25.776706    9137 kubeadm.go:581] duration metric: took 11.426545329s to wait for : map[apiserver:true system_pods:true] ...
I1105 20:38:25.776719    9137 node_conditions.go:102] verifying NodePressure condition ...
I1105 20:38:25.784885    9137 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1105 20:38:25.784897    9137 node_conditions.go:123] node cpu capacity is 4
I1105 20:38:25.784906    9137 node_conditions.go:105] duration metric: took 8.183035ms to run NodePressure ...
I1105 20:38:25.784915    9137 start.go:228] waiting for startup goroutines ...
I1105 20:38:25.784920    9137 start.go:233] waiting for cluster config update ...
I1105 20:38:25.784928    9137 start.go:242] writing updated cluster config ...
I1105 20:38:25.785196    9137 ssh_runner.go:195] Run: rm -f paused
I1105 20:38:26.160871    9137 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I1105 20:38:26.169115    9137 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 05 13:37:40 minikube cri-dockerd[1189]: time="2023-11-05T13:37:40Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 05 13:37:40 minikube cri-dockerd[1189]: time="2023-11-05T13:37:40Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 05 13:37:40 minikube cri-dockerd[1189]: time="2023-11-05T13:37:40Z" level=info msg="Start cri-dockerd grpc backend"
Nov 05 13:37:41 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-db-0_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"959c9da2214bf5b7da94c8014ed7710677c7626fa1426e76bc5ccfa0924c1013\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-db-0_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9e7667f5f3d4d559b4e3eba5a46e09e859d8bf892507e82e1b15c78a2affb96e\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-backend-b8676b5d5-h2f4c_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"199f2171e4ccc92a14211d989ffcc813fce22b0d7ead4502b26671bed8dd03ba\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-backend-b8676b5d5-h2f4c_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2f91fbcdc50630fabd2378493402adfd38b2e7b7128e1cc02645bb515160610c\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-6hfcj_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cac5076246664d97b6358bda8cde9ce10c48ba2f679099b7abdb5f48aa87b9ab\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-6hfcj_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9b6de744c44dbc06b89d23e3fc613df1fffab6dcfaa3821cfe07244a5aaf16d0\""
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"fdcaea70312f43d2973e1372ff59f54b326d9be3b2d023079128f147ec1a05f5\". Proceed without further sandbox information."
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"238674bc497e99b5950550603078f22c193667e901defc39e6c6f921674b98c5\". Proceed without further sandbox information."
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a2a12ab837d26c4b1f1f48e4bd626d68c37bd35a9dbb2220ba9019f0c460b653\". Proceed without further sandbox information."
Nov 05 13:37:56 minikube cri-dockerd[1189]: time="2023-11-05T13:37:56Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"e2494c20c49b5f06eb42d0593e71e2bc86a43bee25e1dde6b710256be4c91555\". Proceed without further sandbox information."
Nov 05 13:37:57 minikube cri-dockerd[1189]: time="2023-11-05T13:37:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e77f05fa169c169675af6384b4b5152691185b93ebf746136489036a922b3265/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:37:57 minikube cri-dockerd[1189]: time="2023-11-05T13:37:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5df28eeff7f626bb22b3437410f0192799fc9f41c51ed4fe0fa7f38d371d60a1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:37:57 minikube cri-dockerd[1189]: time="2023-11-05T13:37:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d9196f4da0077ca5092b6416b0b985b0932631c84351495cf7d0eec392337d48/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:37:57 minikube cri-dockerd[1189]: time="2023-11-05T13:37:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3cd0cd2f314eb4ce00830447b8a793fbcbdef5fba70d0a60142159d4d8faa641/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:37:57 minikube cri-dockerd[1189]: time="2023-11-05T13:37:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-backend-b8676b5d5-h2f4c_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"199f2171e4ccc92a14211d989ffcc813fce22b0d7ead4502b26671bed8dd03ba\""
Nov 05 13:37:58 minikube cri-dockerd[1189]: time="2023-11-05T13:37:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-db-0_karsajobs-app\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"959c9da2214bf5b7da94c8014ed7710677c7626fa1426e76bc5ccfa0924c1013\""
Nov 05 13:37:58 minikube cri-dockerd[1189]: time="2023-11-05T13:37:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-6hfcj_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cac5076246664d97b6358bda8cde9ce10c48ba2f679099b7abdb5f48aa87b9ab\""
Nov 05 13:38:05 minikube cri-dockerd[1189]: time="2023-11-05T13:38:05Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 05 13:38:09 minikube cri-dockerd[1189]: time="2023-11-05T13:38:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0efd81871286371add197e13469ac1d8b63c3d5c497b312e7cdba52b16357c13/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:38:09 minikube cri-dockerd[1189]: time="2023-11-05T13:38:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cb37ebc4169ee1d46e525779e68938c6f1dca57ecf92892181a82291954e530b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:38:12 minikube cri-dockerd[1189]: time="2023-11-05T13:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/80ae6d072dbf637ce946e84241f74ed6d32b91498d6cc2301495f7ecc4998d69/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 13:38:12 minikube cri-dockerd[1189]: time="2023-11-05T13:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/30a1287f45f9970f6c98031ccff41170ada1792973f12303c1ae25515d25eaaa/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 13:38:12 minikube cri-dockerd[1189]: time="2023-11-05T13:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f3823f774019f0d91bbb50d275eaa8cbd46785e85c6e909751888b1ce04bbab/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 05 13:38:17 minikube cri-dockerd[1189]: time="2023-11-05T13:38:17Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs:latest"
Nov 05 13:38:22 minikube cri-dockerd[1189]: time="2023-11-05T13:38:22Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Nov 05 13:38:24 minikube dockerd[955]: time="2023-11-05T13:38:24.618438073Z" level=info msg="ignoring event" container=7121924e9151a21b8a242f1662f4896e3ca6c19f0fd229ff94d9ef86100797d6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 13:51:54 minikube cri-dockerd[1189]: time="2023-11-05T13:51:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e5c0653925341886f2bd83d9da13259684b758071e69ba06297a2f4549f7bf6/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 13:52:06 minikube cri-dockerd[1189]: time="2023-11-05T13:52:06Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: ae0b9ce7749b: Downloading [========================>                          ]    1.3MB/2.624MB"
Nov 05 13:52:16 minikube cri-dockerd[1189]: time="2023-11-05T13:52:16Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Downloading [=================>                                 ]  23.05MB/67.76MB"
Nov 05 13:52:26 minikube cri-dockerd[1189]: time="2023-11-05T13:52:26Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Downloading [===============================================>   ]   64.2MB/67.76MB"
Nov 05 13:52:36 minikube cri-dockerd[1189]: time="2023-11-05T13:52:36Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Extracting [==>                                                ]  2.785MB/67.76MB"
Nov 05 13:52:46 minikube cri-dockerd[1189]: time="2023-11-05T13:52:46Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Extracting [=======>                                           ]  10.58MB/67.76MB"
Nov 05 13:52:56 minikube cri-dockerd[1189]: time="2023-11-05T13:52:56Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Extracting [=============>                                     ]  18.94MB/67.76MB"
Nov 05 13:53:06 minikube cri-dockerd[1189]: time="2023-11-05T13:53:06Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Extracting [====================>                              ]  27.85MB/67.76MB"
Nov 05 13:53:16 minikube cri-dockerd[1189]: time="2023-11-05T13:53:16Z" level=info msg="Pulling image ghcr.io/mueiya/karsajobs-ui:latest: fe8fb388c99b: Extracting [===================================>               ]  48.46MB/67.76MB"
Nov 05 13:53:24 minikube cri-dockerd[1189]: time="2023-11-05T13:53:24Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Downloaded newer image for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:06:33 minikube dockerd[955]: time="2023-11-05T14:06:33.815535643Z" level=info msg="ignoring event" container=750dbb12dfa73b5136e138e02f30ae58a9572c0bf06b82a152cfd0f5380cb530 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:06:34 minikube dockerd[955]: time="2023-11-05T14:06:34.601444089Z" level=info msg="ignoring event" container=8e5c0653925341886f2bd83d9da13259684b758071e69ba06297a2f4549f7bf6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:06:34 minikube cri-dockerd[1189]: time="2023-11-05T14:06:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dea3aadbc146d17c0efa45f81ba4f9b54a826e76a8f463c6deb12b716c8bed0d/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 14:06:36 minikube cri-dockerd[1189]: time="2023-11-05T14:06:36Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:07:17 minikube dockerd[955]: time="2023-11-05T14:07:17.492032546Z" level=info msg="ignoring event" container=27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:07:17 minikube dockerd[955]: time="2023-11-05T14:07:17.764986553Z" level=info msg="ignoring event" container=dea3aadbc146d17c0efa45f81ba4f9b54a826e76a8f463c6deb12b716c8bed0d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:07:17 minikube cri-dockerd[1189]: time="2023-11-05T14:07:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/330a7f73a0c0fb1db3c2a8499e8ef2239aa0ec978634bdb218a4cfa643e9db78/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 14:07:20 minikube cri-dockerd[1189]: time="2023-11-05T14:07:20Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:09:02 minikube dockerd[955]: time="2023-11-05T14:09:02.080375605Z" level=info msg="ignoring event" container=07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:09:02 minikube dockerd[955]: time="2023-11-05T14:09:02.329050583Z" level=info msg="ignoring event" container=330a7f73a0c0fb1db3c2a8499e8ef2239aa0ec978634bdb218a4cfa643e9db78 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:09:47 minikube cri-dockerd[1189]: time="2023-11-05T14:09:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/994af6b9c7533f219748c4b0bc4495192b918c29ec3da9170165d87433f5e46b/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 14:09:49 minikube cri-dockerd[1189]: time="2023-11-05T14:09:49Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:21:22 minikube dockerd[955]: time="2023-11-05T14:21:22.705654858Z" level=info msg="ignoring event" container=4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:21:22 minikube cri-dockerd[1189]: time="2023-11-05T14:21:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/501b55f9e9c5350101e61e3dcc49421b733f7230a0e6611ada61e01440fa40af/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 14:21:23 minikube dockerd[955]: time="2023-11-05T14:21:23.568972747Z" level=info msg="ignoring event" container=994af6b9c7533f219748c4b0bc4495192b918c29ec3da9170165d87433f5e46b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:21:25 minikube cri-dockerd[1189]: time="2023-11-05T14:21:25Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:22:13 minikube cri-dockerd[1189]: time="2023-11-05T14:22:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7cba0655d7104b892484c2496e89a91e870f7189232c3cf5874df88e604375ef/resolv.conf as [nameserver 10.96.0.10 search karsajobs-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 05 14:22:15 minikube cri-dockerd[1189]: time="2023-11-05T14:22:15Z" level=info msg="Stop pulling image ghcr.io/mueiya/karsajobs-ui:latest: Status: Image is up to date for ghcr.io/mueiya/karsajobs-ui:latest"
Nov 05 14:22:17 minikube dockerd[955]: time="2023-11-05T14:22:17.910879738Z" level=info msg="ignoring event" container=4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 14:22:18 minikube dockerd[955]: time="2023-11-05T14:22:18.332867131Z" level=info msg="ignoring event" container=501b55f9e9c5350101e61e3dcc49421b733f7230a0e6611ada61e01440fa40af module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                 CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
52cdaff5e54eb       ghcr.io/mueiya/karsajobs-ui@sha256:90ef7b03f0a3450799f5368a76d9dea5023c02a403735e52266886d6af34d778   8 minutes ago       Running             karsajobs-ui              0                   7cba0655d7104       karsajobs-ui-6c64569b8f-nnr74
90b7e062a301d       6e38f40d628db                                                                                         52 minutes ago      Running             storage-provisioner       7                   0efd818712863       storage-provisioner
5165438def6e4       mongo@sha256:d341a86584b96eb665345a8f5b35fba8695ee1d0618fd012ec4696223a3d6c62                         52 minutes ago      Running             mongo                     2                   80ae6d072dbf6       karsajobs-db-0
7a9d5a0cc3cf6       ghcr.io/mueiya/karsajobs@sha256:b762c769c91811bc458791ac831c019aa9e856c6cf2adfb5a95c5c6c56cd0214      52 minutes ago      Running             karsajobs-backend         2                   30a1287f45f99       karsajobs-backend-b8676b5d5-h2f4c
df189146d06dc       ead0a4a53df89                                                                                         52 minutes ago      Running             coredns                   3                   7f3823f774019       coredns-5d78c9869d-6hfcj
7121924e9151a       6e38f40d628db                                                                                         52 minutes ago      Exited              storage-provisioner       6                   0efd818712863       storage-provisioner
1385ee9e05834       6848d7eda0341                                                                                         52 minutes ago      Running             kube-proxy                3                   cb37ebc4169ee       kube-proxy-mm465
9548fde391525       98ef2570f3cde                                                                                         52 minutes ago      Running             kube-scheduler            3                   e77f05fa169c1       kube-scheduler-minikube
f5f3e813f1f01       e7972205b6614                                                                                         52 minutes ago      Running             kube-apiserver            3                   5df28eeff7f62       kube-apiserver-minikube
d0c91706c2213       86b6af7dd652c                                                                                         52 minutes ago      Running             etcd                      3                   d9196f4da0077       etcd-minikube
468f2f7cd9a42       f466468864b7a                                                                                         52 minutes ago      Running             kube-controller-manager   3                   3cd0cd2f314eb       kube-controller-manager-minikube
c3ee1a1c17634       mongo@sha256:d341a86584b96eb665345a8f5b35fba8695ee1d0618fd012ec4696223a3d6c62                         About an hour ago   Exited              mongo                     1                   959c9da2214bf       karsajobs-db-0
f4bed68bc07a3       ghcr.io/mueiya/karsajobs@sha256:b762c769c91811bc458791ac831c019aa9e856c6cf2adfb5a95c5c6c56cd0214      About an hour ago   Exited              karsajobs-backend         1                   199f2171e4ccc       karsajobs-backend-b8676b5d5-h2f4c
6a51ba832c7b5       ead0a4a53df89                                                                                         About an hour ago   Exited              coredns                   2                   cac5076246664       coredns-5d78c9869d-6hfcj
2e205cd7fc79b       6848d7eda0341                                                                                         About an hour ago   Exited              kube-proxy                2                   45cb9a138182c       kube-proxy-mm465
310f2eae0f35c       e7972205b6614                                                                                         About an hour ago   Exited              kube-apiserver            2                   09d86773b284d       kube-apiserver-minikube
b38dbc622c935       86b6af7dd652c                                                                                         About an hour ago   Exited              etcd                      2                   8ef75b356b0b1       etcd-minikube
f2081a31d0a6d       f466468864b7a                                                                                         About an hour ago   Exited              kube-controller-manager   2                   52d69bc482e3a       kube-controller-manager-minikube
ca393e85decbd       98ef2570f3cde                                                                                         About an hour ago   Exited              kube-scheduler            2                   51ceb18752876       kube-scheduler-minikube

* 
* ==> coredns [6a51ba832c7b] <==
* [INFO] 10.244.0.7:38190 - 60067 "A IN karsajobs-db. udp 30 false 512" - - 0 2.180694383s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:59360->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:38139 - 44779 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000160881s
[INFO] 10.244.0.7:52065 - 29694 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000221881s
[INFO] 10.244.0.7:56953 - 1764 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000231368s
[INFO] 10.244.0.7:60930 - 33071 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000302164s
[INFO] 10.244.0.7:49393 - 41579 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000125683s
[INFO] 10.244.0.7:52581 - 34727 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000076969s
[INFO] 10.244.0.7:32777 - 20721 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000680783s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:44811->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:57165 - 57704 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000702505s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:46672->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:51457 - 37196 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000594031s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:44221->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:42192 - 7139 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000637689s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:59741->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:59489 - 37140 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000156379s
[INFO] 10.244.0.7:52336 - 964 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000056208s
[INFO] 10.244.0.7:44663 - 64240 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000064002s
[INFO] 10.244.0.7:43765 - 31961 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000023786s
[INFO] 10.244.0.7:40150 - 32432 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000097877s
[INFO] 10.244.0.7:46357 - 17282 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000147822s
[INFO] 10.244.0.7:50355 - 51463 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001526206s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:50641->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:36176 - 50343 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001566702s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:45701->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:39744 - 19354 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000711002s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:57161->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:48196 - 50257 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000781213s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:59364->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:36881 - 18547 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000135958s
[INFO] 10.244.0.7:40850 - 49308 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000135838s
[INFO] 10.244.0.7:35532 - 24629 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000146178s
[INFO] 10.244.0.7:35883 - 19808 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000184475s
[INFO] 10.244.0.7:51310 - 16000 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000108677s
[INFO] 10.244.0.7:50032 - 26590 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.00028955s
[INFO] 10.244.0.7:57761 - 40525 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001425952s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:35565->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:55038 - 53261 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.006417213s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:44337->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:57189 - 55739 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000665073s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:54232->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:55088 - 42108 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.00112141s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:47221->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:34967 - 29015 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00014191s
[INFO] 10.244.0.7:51334 - 30253 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000204822s
[INFO] 10.244.0.7:34555 - 38248 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.00015741s
[INFO] 10.244.0.7:36798 - 2169 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000070624s
[INFO] 10.244.0.7:49950 - 26201 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000112484s
[INFO] 10.244.0.7:35666 - 19753 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000158745s
[INFO] 10.244.0.7:51310 - 1366 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000815943s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:60929->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:52329 - 54579 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000878722s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:53200->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:42657 - 59627 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000933746s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.8:39866->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:38952 - 45022 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000902239s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.8:37452->192.168.65.254:53: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [df189146d06d] <==
* [INFO] 10.244.0.11:51553 - 20046 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000127243s
[INFO] 10.244.0.11:41029 - 21504 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000209545s
[INFO] 10.244.0.11:33456 - 26764 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000076362s
[INFO] 10.244.0.11:40271 - 32216 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000115585s
[INFO] 10.244.0.11:60238 - 56303 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.0013920609999998s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:59043->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:47536 - 33210 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001412163s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:35260->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:39903 - 46078 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001245169s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:49970->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:40032 - 60436 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001281007s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:46914->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:45312 - 13809 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000144337s
[INFO] 10.244.0.11:47263 - 8281 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000248165s
[INFO] 10.244.0.11:51309 - 63593 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000062991s
[INFO] 10.244.0.11:50171 - 8016 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.00007833s
[INFO] 10.244.0.11:39878 - 39779 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000086315s
[INFO] 10.244.0.11:37140 - 36476 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000129548s
[INFO] 10.244.0.11:50419 - 34628 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.000730806s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:38661->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:36132 - 52058 "A IN karsajobs-db. udp 30 false 512" - - 0 2.004973675s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:57231->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:55024 - 19540 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001063689s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:33180->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:59028 - 52075 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001097886s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:55967->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:34640 - 15698 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000183851s
[INFO] 10.244.0.11:45192 - 36154 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00026677s
[INFO] 10.244.0.11:33485 - 28823 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000077217s
[INFO] 10.244.0.11:58591 - 7702 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000054574s
[INFO] 10.244.0.11:54937 - 19991 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000054534s
[INFO] 10.244.0.11:59809 - 4177 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000053352s
[INFO] 10.244.0.11:50649 - 59640 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000793684s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:48474->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:51755 - 5122 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.00099628s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:33265->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:43114 - 20008 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001034646s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:55187->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:54862 - 7233 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001355028s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:44439->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:35775 - 19251 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00013602s
[INFO] 10.244.0.11:51413 - 31598 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000211459s
[INFO] 10.244.0.11:41895 - 27179 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000168763s
[INFO] 10.244.0.11:46867 - 58819 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000159334s
[INFO] 10.244.0.11:53782 - 6624 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.00008371s
[INFO] 10.244.0.11:44924 - 60448 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000065886s
[INFO] 10.244.0.11:35335 - 37671 "A IN karsajobs-db. udp 30 false 512" - - 0 2.000366841s
[INFO] 10.244.0.11:44610 - 31911 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001150632s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:59795->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:50042->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:51485 - 1801 "A IN karsajobs-db. udp 30 false 512" - - 0 2.001236281s
[ERROR] plugin/errors: 2 karsajobs-db. A: read udp 10.244.0.12:34625->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:35058 - 39766 "AAAA IN karsajobs-db. udp 30 false 512" - - 0 2.001368989s
[ERROR] plugin/errors: 2 karsajobs-db. AAAA: read udp 10.244.0.12:43761->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.11:56824 - 58760 "AAAA IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000082237s
[INFO] 10.244.0.11:33982 - 25771 "A IN karsajobs-db.karsajobs-app.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00006268s
[INFO] 10.244.0.11:44600 - 43172 "AAAA IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000080734s
[INFO] 10.244.0.11:57083 - 63534 "A IN karsajobs-db.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000113034s
[INFO] 10.244.0.11:59360 - 7859 "A IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000126932s
[INFO] 10.244.0.11:48666 - 26281 "AAAA IN karsajobs-db.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000048583s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_10_26T12_43_41_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 26 Oct 2023 05:43:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 05 Nov 2023 14:30:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 05 Nov 2023 14:29:25 +0000   Thu, 26 Oct 2023 05:43:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 05 Nov 2023 14:29:25 +0000   Thu, 26 Oct 2023 05:43:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 05 Nov 2023 14:29:25 +0000   Thu, 26 Oct 2023 05:43:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 05 Nov 2023 14:29:25 +0000   Thu, 26 Oct 2023 05:43:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7068432Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7068432Ki
  pods:               110
System Info:
  Machine ID:                 314dbbd37f6e43938fae52b2d40e21de
  System UUID:                314dbbd37f6e43938fae52b2d40e21de
  Boot ID:                    bf2601f0-bb14-48a2-af25-245928782473
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  karsajobs-app               karsajobs-backend-b8676b5d5-h2f4c    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  karsajobs-app               karsajobs-db-0                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  karsajobs-app               karsajobs-ui-6c64569b8f-nnr74        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m30s
  kube-system                 coredns-5d78c9869d-6hfcj             100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     10d
  kube-system                 etcd-minikube                        100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         10d
  kube-system                 kube-apiserver-minikube              250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  kube-system                 kube-controller-manager-minikube     200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  kube-system                 kube-proxy-mm465                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  kube-system                 kube-scheduler-minikube              100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  kube-system                 storage-provisioner                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 69m                kube-proxy       
  Normal  Starting                 52m                kube-proxy       
  Normal  NodeAllocatableEnforced  69m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  69m (x8 over 69m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    69m (x8 over 69m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     69m (x7 over 69m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 69m                kubelet          Starting kubelet.
  Normal  RegisteredNode           69m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 52m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  52m (x8 over 52m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    52m (x8 over 52m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     52m (x7 over 52m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  52m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           52m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.003520] blk_update_request: I/O error, dev sdd, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.069013] Buffer I/O error on dev sdd, logical block 134184960, lost sync page write
[  +0.001319] JBD2: Error -5 detected when updating journal superblock for sdd-8.
[  +0.001044] Aborting journal on device sdd-8.
[  +0.011376] blk_update_request: I/O error, dev sdd, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.004833] blk_update_request: I/O error, dev sdd, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.012346] Buffer I/O error on dev sdd, logical block 134184960, lost sync page write
[  +0.012980] JBD2: Error -5 detected when updating journal superblock for sdd-8.
[  +0.002104] EXT4-fs error (device sdd): ext4_put_super:1188: comm weston: Couldn't clean up the journal
[  +0.001876] EXT4-fs (sdd): Remounting filesystem read-only
[  +2.702023] FS-Cache: Duplicate cookie detected
[  +0.000744] FS-Cache: O-cookie c=00000033 [p=00000002 fl=222 nc=0 na=1]
[  +0.000565] FS-Cache: O-cookie d=00000000b068bee3{9P.session} n=000000001e5915b5
[  +0.000711] FS-Cache: O-key=[10] '34323934393739383635'
[  +0.000661] FS-Cache: N-cookie c=00000034 [p=00000002 fl=2 nc=0 na=1]
[  +0.000570] FS-Cache: N-cookie d=00000000b068bee3{9P.session} n=00000000b5003e5f
[  +0.000653] FS-Cache: N-key=[10] '34323934393739383635'
[  +0.044324] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.023985] FS-Cache: Duplicate cookie detected
[  +0.003854] FS-Cache: O-cookie c=00000036 [p=00000002 fl=222 nc=0 na=1]
[  +0.000537] FS-Cache: O-cookie d=00000000b068bee3{9P.session} n=00000000eb7c5816
[  +0.001664] FS-Cache: O-key=[10] '34323934393739383732'
[  +0.000611] FS-Cache: N-cookie c=00000037 [p=00000002 fl=2 nc=0 na=1]
[  +0.000749] FS-Cache: N-cookie d=00000000b068bee3{9P.session} n=00000000598eb326
[  +0.001235] FS-Cache: N-key=[10] '34323934393739383732'
[  +0.011638] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +0.163866] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.012739] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001230] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006929] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +7.187823] 9pnet_virtio: no channels available for device drvfs
[  +0.007255] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100111] 9pnet_virtio: no channels available for device drvfs
[  +0.119278] 9pnet_virtio: no channels available for device drvfs
[  +0.151139] 9pnet_virtio: no channels available for device drvfs
[  +0.000745] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100192] 9pnet_virtio: no channels available for device drvfs
[  +0.112032] 9pnet_virtio: no channels available for device drvfs
[  +0.512182] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.001069] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.021150] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.070881] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.449091] 9pnet_virtio: no channels available for device drvfs
[  +0.039366] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100172] 9pnet_virtio: no channels available for device drvfs
[  +0.107071] 9pnet_virtio: no channels available for device drvfs
[  +0.038443] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.009874] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.011583] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.119946] 9pnet_virtio: no channels available for device drvfs
[  +0.003255] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.107289] 9pnet_virtio: no channels available for device drvfs
[  +0.110305] 9pnet_virtio: no channels available for device drvfs
[  +0.173225] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +2.234508] new mount options do not match the existing superblock, will be ignored
[Nov 5 13:38] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.

* 
* ==> etcd [b38dbc622c93] <==
* {"level":"info","ts":"2023-11-05T13:21:12.541Z","caller":"traceutil/trace.go:171","msg":"trace[1319605213] range","detail":"{range_begin:/registry/pods/kube-system/kube-scheduler-minikube; range_end:; response_count:1; response_revision:6149; }","duration":"353.069386ms","start":"2023-11-05T13:21:12.188Z","end":"2023-11-05T13:21:12.541Z","steps":["trace[1319605213] 'agreement among raft nodes before linearized reading'  (duration: 352.909532ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:21:12.541Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:21:12.188Z","time spent":"353.120854ms","remote":"127.0.0.1:59594","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":4411,"request content":"key:\"/registry/pods/kube-system/kube-scheduler-minikube\" "}
{"level":"warn","ts":"2023-11-05T13:21:15.149Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.159899ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024945277370581 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:6071 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2023-11-05T13:21:15.149Z","caller":"traceutil/trace.go:171","msg":"trace[1595107366] transaction","detail":"{read_only:false; response_revision:6158; number_of_response:1; }","duration":"194.654675ms","start":"2023-11-05T13:21:14.954Z","end":"2023-11-05T13:21:15.149Z","steps":["trace[1595107366] 'process raft request'  (duration: 94.255572ms)","trace[1595107366] 'compare'  (duration: 100.042967ms)"],"step_count":2}
{"level":"info","ts":"2023-11-05T13:21:15.402Z","caller":"traceutil/trace.go:171","msg":"trace[1877838106] transaction","detail":"{read_only:false; response_revision:6159; number_of_response:1; }","duration":"151.906783ms","start":"2023-11-05T13:21:15.250Z","end":"2023-11-05T13:21:15.402Z","steps":["trace[1877838106] 'process raft request'  (duration: 97.724685ms)","trace[1877838106] 'compare'  (duration: 53.933856ms)"],"step_count":2}
{"level":"info","ts":"2023-11-05T13:21:16.348Z","caller":"traceutil/trace.go:171","msg":"trace[713441479] transaction","detail":"{read_only:false; response_revision:6161; number_of_response:1; }","duration":"156.895692ms","start":"2023-11-05T13:21:16.191Z","end":"2023-11-05T13:21:16.348Z","steps":["trace[713441479] 'process raft request'  (duration: 156.75222ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:21:24.551Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.456369629s","expected-duration":"1s"}
{"level":"info","ts":"2023-11-05T13:21:24.551Z","caller":"traceutil/trace.go:171","msg":"trace[992650992] transaction","detail":"{read_only:false; response_revision:6188; number_of_response:1; }","duration":"1.45691819s","start":"2023-11-05T13:21:23.758Z","end":"2023-11-05T13:21:24.551Z","steps":["trace[992650992] 'process raft request'  (duration: 1.45677035s)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:21:24.551Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:21:23.758Z","time spent":"1.457004684s","remote":"127.0.0.1:59498","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":764,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.1794bcbe05ea10d5\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.1794bcbe05ea10d5\" value_size:681 lease:8128024945277370232 >> failure:<>"}
{"level":"info","ts":"2023-11-05T13:21:24.672Z","caller":"traceutil/trace.go:171","msg":"trace[617642118] transaction","detail":"{read_only:false; response_revision:6190; number_of_response:1; }","duration":"130.720754ms","start":"2023-11-05T13:21:24.541Z","end":"2023-11-05T13:21:24.672Z","steps":["trace[617642118] 'process raft request'  (duration: 130.675949ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:21:24.672Z","caller":"traceutil/trace.go:171","msg":"trace[1513108043] transaction","detail":"{read_only:false; response_revision:6189; number_of_response:1; }","duration":"1.561164788s","start":"2023-11-05T13:21:23.774Z","end":"2023-11-05T13:21:24.672Z","steps":["trace[1513108043] 'process raft request'  (duration: 1.560950021s)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:21:24.672Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:21:23.774Z","time spent":"1.561498392s","remote":"127.0.0.1:59594","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4207,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/storage-provisioner\" mod_revision:6148 > success:<request_put:<key:\"/registry/pods/kube-system/storage-provisioner\" value_size:4153 >> failure:<request_range:<key:\"/registry/pods/kube-system/storage-provisioner\" > >"}
{"level":"info","ts":"2023-11-05T13:22:56.311Z","caller":"traceutil/trace.go:171","msg":"trace[1620690065] transaction","detail":"{read_only:false; response_revision:6254; number_of_response:1; }","duration":"213.428733ms","start":"2023-11-05T13:22:56.097Z","end":"2023-11-05T13:22:56.311Z","steps":["trace[1620690065] 'process raft request'  (duration: 213.325207ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:28:01.764Z","caller":"traceutil/trace.go:171","msg":"trace[501728713] transaction","detail":"{read_only:false; response_revision:6497; number_of_response:1; }","duration":"117.793608ms","start":"2023-11-05T13:28:01.645Z","end":"2023-11-05T13:28:01.763Z","steps":["trace[501728713] 'process raft request'  (duration: 117.656417ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:30:59.126Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6399}
{"level":"info","ts":"2023-11-05T13:30:59.150Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6399,"took":"22.98475ms","hash":1449875512}
{"level":"info","ts":"2023-11-05T13:30:59.150Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1449875512,"revision":6399,"compact-revision":5790}
{"level":"info","ts":"2023-11-05T13:31:06.526Z","caller":"traceutil/trace.go:171","msg":"trace[1580544426] transaction","detail":"{read_only:false; response_revision:6643; number_of_response:1; }","duration":"103.367177ms","start":"2023-11-05T13:31:06.423Z","end":"2023-11-05T13:31:06.526Z","steps":["trace[1580544426] 'process raft request'  (duration: 103.276064ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:32:47.215Z","caller":"traceutil/trace.go:171","msg":"trace[239158963] linearizableReadLoop","detail":"{readStateIndex:8314; appliedIndex:8313; }","duration":"154.976603ms","start":"2023-11-05T13:32:47.058Z","end":"2023-11-05T13:32:47.213Z","steps":["trace[239158963] 'read index received'  (duration: 48.252333ms)","trace[239158963] 'applied index is now lower than readState.Index'  (duration: 106.719962ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-05T13:32:47.225Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.081052ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:32:47.225Z","caller":"traceutil/trace.go:171","msg":"trace[1490293964] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6723; }","duration":"166.872048ms","start":"2023-11-05T13:32:47.058Z","end":"2023-11-05T13:32:47.225Z","steps":["trace[1490293964] 'agreement among raft nodes before linearized reading'  (duration: 154.992365ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:52.874Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"267.0062ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024945277373488 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6728 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2023-11-05T13:32:52.875Z","caller":"traceutil/trace.go:171","msg":"trace[2131992976] linearizableReadLoop","detail":"{readStateIndex:8321; appliedIndex:8320; }","duration":"568.562246ms","start":"2023-11-05T13:32:52.306Z","end":"2023-11-05T13:32:52.875Z","steps":["trace[2131992976] 'read index received'  (duration: 300.169914ms)","trace[2131992976] 'applied index is now lower than readState.Index'  (duration: 268.390469ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-05T13:32:52.875Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"568.742179ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:32:52.875Z","caller":"traceutil/trace.go:171","msg":"trace[731522151] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6729; }","duration":"568.781954ms","start":"2023-11-05T13:32:52.306Z","end":"2023-11-05T13:32:52.875Z","steps":["trace[731522151] 'agreement among raft nodes before linearized reading'  (duration: 568.64324ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:52.875Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:32:52.306Z","time spent":"568.861847ms","remote":"127.0.0.1:59802","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-11-05T13:32:52.875Z","caller":"traceutil/trace.go:171","msg":"trace[2107591348] transaction","detail":"{read_only:false; response_revision:6729; number_of_response:1; }","duration":"666.082047ms","start":"2023-11-05T13:32:52.209Z","end":"2023-11-05T13:32:52.875Z","steps":["trace[2107591348] 'process raft request'  (duration: 396.986082ms)","trace[2107591348] 'compare'  (duration: 266.762897ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-05T13:32:52.875Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:32:52.209Z","time spent":"666.15662ms","remote":"127.0.0.1:59572","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6728 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-11-05T13:32:55.731Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024945277373497,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-05T13:32:55.897Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.013648576s","expected-duration":"1s"}
{"level":"info","ts":"2023-11-05T13:32:55.897Z","caller":"traceutil/trace.go:171","msg":"trace[1418044929] linearizableReadLoop","detail":"{readStateIndex:8322; appliedIndex:8321; }","duration":"849.945963ms","start":"2023-11-05T13:32:55.047Z","end":"2023-11-05T13:32:55.897Z","steps":["trace[1418044929] 'read index received'  (duration: 849.907975ms)","trace[1418044929] 'applied index is now lower than readState.Index'  (duration: 37.507¬µs)"],"step_count":2}
{"level":"info","ts":"2023-11-05T13:32:55.897Z","caller":"traceutil/trace.go:171","msg":"trace[1188369897] transaction","detail":"{read_only:false; response_revision:6730; number_of_response:1; }","duration":"1.014619232s","start":"2023-11-05T13:32:54.883Z","end":"2023-11-05T13:32:55.897Z","steps":["trace[1188369897] 'process raft request'  (duration: 1.014305245s)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:55.897Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:32:54.883Z","time spent":"1.014669239s","remote":"127.0.0.1:59572","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6729 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-11-05T13:32:55.898Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"850.525464ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:32:55.898Z","caller":"traceutil/trace.go:171","msg":"trace[329065887] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6730; }","duration":"850.550092ms","start":"2023-11-05T13:32:55.047Z","end":"2023-11-05T13:32:55.898Z","steps":["trace[329065887] 'agreement among raft nodes before linearized reading'  (duration: 850.48167ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:55.898Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:32:55.047Z","time spent":"850.59059ms","remote":"127.0.0.1:59802","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-11-05T13:32:55.898Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.081356ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:32:55.898Z","caller":"traceutil/trace.go:171","msg":"trace[359198113] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:6730; }","duration":"166.102316ms","start":"2023-11-05T13:32:55.732Z","end":"2023-11-05T13:32:55.898Z","steps":["trace[359198113] 'agreement among raft nodes before linearized reading'  (duration: 166.054113ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:57.306Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"295.226446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/kubernetes\" ","response":"range_response_count:1 size:704"}
{"level":"info","ts":"2023-11-05T13:32:57.412Z","caller":"traceutil/trace.go:171","msg":"trace[470143070] range","detail":"{range_begin:/registry/services/specs/default/kubernetes; range_end:; response_count:1; response_revision:6730; }","duration":"295.303154ms","start":"2023-11-05T13:32:57.011Z","end":"2023-11-05T13:32:57.306Z","steps":["trace[470143070] 'range keys from bolt db'  (duration: 295.029455ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:32:57.652Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:32:57.011Z","time spent":"640.91881ms","remote":"127.0.0.1:59640","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":728,"request content":"key:\"/registry/services/specs/default/kubernetes\" "}
{"level":"warn","ts":"2023-11-05T13:33:04.412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"533.22849ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:60193"}
{"level":"info","ts":"2023-11-05T13:33:04.412Z","caller":"traceutil/trace.go:171","msg":"trace[119038802] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:6736; }","duration":"533.378748ms","start":"2023-11-05T13:33:03.879Z","end":"2023-11-05T13:33:04.412Z","steps":["trace[119038802] 'range keys from bolt db'  (duration: 533.165791ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:33:04.412Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:33:03.879Z","time spent":"533.426959ms","remote":"127.0.0.1:59610","response type":"/etcdserverpb.KV/Range","request count":0,"request size":29,"response count":1,"response size":60217,"request content":"key:\"/registry/ranges/serviceips\" "}
{"level":"info","ts":"2023-11-05T13:33:04.415Z","caller":"traceutil/trace.go:171","msg":"trace[1972107361] transaction","detail":"{read_only:false; response_revision:6737; number_of_response:1; }","duration":"311.321944ms","start":"2023-11-05T13:33:04.103Z","end":"2023-11-05T13:33:04.415Z","steps":["trace[1972107361] 'process raft request'  (duration: 311.044539ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:33:04.417Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:33:04.103Z","time spent":"313.999196ms","remote":"127.0.0.1:59572","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6736 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-05T13:33:08.238Z","caller":"traceutil/trace.go:171","msg":"trace[1279332936] linearizableReadLoop","detail":"{readStateIndex:8332; appliedIndex:8331; }","duration":"366.094785ms","start":"2023-11-05T13:33:07.872Z","end":"2023-11-05T13:33:08.238Z","steps":["trace[1279332936] 'read index received'  (duration: 364.753898ms)","trace[1279332936] 'applied index is now lower than readState.Index'  (duration: 1.339835ms)"],"step_count":2}
{"level":"info","ts":"2023-11-05T13:33:08.238Z","caller":"traceutil/trace.go:171","msg":"trace[403754551] transaction","detail":"{read_only:false; response_revision:6738; number_of_response:1; }","duration":"490.895654ms","start":"2023-11-05T13:33:07.747Z","end":"2023-11-05T13:33:08.238Z","steps":["trace[403754551] 'process raft request'  (duration: 489.573196ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:33:08.238Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:33:07.747Z","time spent":"490.970407ms","remote":"127.0.0.1:59572","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6737 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-11-05T13:33:08.238Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"366.284643ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/kubernetes\" ","response":"range_response_count:1 size:704"}
{"level":"info","ts":"2023-11-05T13:33:08.238Z","caller":"traceutil/trace.go:171","msg":"trace[1698970695] range","detail":"{range_begin:/registry/services/specs/default/kubernetes; range_end:; response_count:1; response_revision:6738; }","duration":"366.315232ms","start":"2023-11-05T13:33:07.872Z","end":"2023-11-05T13:33:08.238Z","steps":["trace[1698970695] 'agreement among raft nodes before linearized reading'  (duration: 366.187548ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:33:08.239Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:33:07.872Z","time spent":"366.349237ms","remote":"127.0.0.1:59640","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":728,"request content":"key:\"/registry/services/specs/default/kubernetes\" "}
{"level":"info","ts":"2023-11-05T13:33:10.377Z","caller":"traceutil/trace.go:171","msg":"trace[1583310850] transaction","detail":"{read_only:false; response_revision:6742; number_of_response:1; }","duration":"129.330148ms","start":"2023-11-05T13:33:10.248Z","end":"2023-11-05T13:33:10.377Z","steps":["trace[1583310850] 'process raft request'  (duration: 129.218084ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:33:20.251Z","caller":"traceutil/trace.go:171","msg":"trace[2054746908] transaction","detail":"{read_only:false; response_revision:6749; number_of_response:1; }","duration":"137.987283ms","start":"2023-11-05T13:33:20.113Z","end":"2023-11-05T13:33:20.251Z","steps":["trace[2054746908] 'process raft request'  (duration: 137.871232ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:33:35.410Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-11-05T13:33:35.412Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-11-05T13:33:35.554Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-05T13:33:35.621Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-05T13:33:35.623Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-05T13:33:35.623Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [d0c91706c221] <==
* {"level":"info","ts":"2023-11-05T13:53:00.472Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7328,"took":"2.106158ms","hash":2862596235}
{"level":"info","ts":"2023-11-05T13:53:00.472Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2862596235,"revision":7328,"compact-revision":7089}
{"level":"warn","ts":"2023-11-05T13:53:26.419Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024945538822805,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-05T13:53:26.923Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024945538822805,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-05T13:53:27.342Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.738141113s","expected-duration":"1s"}
{"level":"info","ts":"2023-11-05T13:53:27.686Z","caller":"traceutil/trace.go:171","msg":"trace[1130493050] transaction","detail":"{read_only:false; response_revision:7606; number_of_response:1; }","duration":"1.838469354s","start":"2023-11-05T13:53:25.848Z","end":"2023-11-05T13:53:27.686Z","steps":["trace[1130493050] 'process raft request'  (duration: 1.775561676s)","trace[1130493050] 'compare'  (duration: 32.028457ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-05T13:53:27.686Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:53:25.848Z","time spent":"1.838590372s","remote":"127.0.0.1:49260","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:7597 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2023-11-05T13:53:27.691Z","caller":"traceutil/trace.go:171","msg":"trace[1737277485] linearizableReadLoop","detail":"{readStateIndex:9412; appliedIndex:9409; }","duration":"1.771915641s","start":"2023-11-05T13:53:25.919Z","end":"2023-11-05T13:53:27.691Z","steps":["trace[1737277485] 'read index received'  (duration: 1.47973448s)","trace[1737277485] 'applied index is now lower than readState.Index'  (duration: 292.180572ms)"],"step_count":2}
{"level":"info","ts":"2023-11-05T13:53:27.691Z","caller":"traceutil/trace.go:171","msg":"trace[625624765] transaction","detail":"{read_only:false; response_revision:7607; number_of_response:1; }","duration":"1.273395813s","start":"2023-11-05T13:53:26.418Z","end":"2023-11-05T13:53:27.691Z","steps":["trace[625624765] 'process raft request'  (duration: 1.267182224s)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:53:27.691Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:53:26.418Z","time spent":"1.273547103s","remote":"127.0.0.1:49154","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4697,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:7343 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:4663 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >"}
{"level":"warn","ts":"2023-11-05T13:53:27.692Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.772673329s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:53:27.692Z","caller":"traceutil/trace.go:171","msg":"trace[748871832] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:7607; }","duration":"1.772709962s","start":"2023-11-05T13:53:25.919Z","end":"2023-11-05T13:53:27.692Z","steps":["trace[748871832] 'agreement among raft nodes before linearized reading'  (duration: 1.772592354s)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:53:27.692Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:53:25.919Z","time spent":"1.772750923s","remote":"127.0.0.1:49212","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":29,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2023-11-05T13:53:27.692Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"539.016815ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/karsajobs-app/mongo-pvc\" ","response":"range_response_count:1 size:1388"}
{"level":"info","ts":"2023-11-05T13:53:27.692Z","caller":"traceutil/trace.go:171","msg":"trace[380449442] range","detail":"{range_begin:/registry/persistentvolumeclaims/karsajobs-app/mongo-pvc; range_end:; response_count:1; response_revision:7607; }","duration":"539.041563ms","start":"2023-11-05T13:53:27.153Z","end":"2023-11-05T13:53:27.692Z","steps":["trace[380449442] 'agreement among raft nodes before linearized reading'  (duration: 538.971207ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:53:27.692Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:53:27.153Z","time spent":"539.15952ms","remote":"127.0.0.1:49118","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":1,"response size":1412,"request content":"key:\"/registry/persistentvolumeclaims/karsajobs-app/mongo-pvc\" "}
{"level":"warn","ts":"2023-11-05T13:53:27.693Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"310.493069ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-05T13:53:27.693Z","caller":"traceutil/trace.go:171","msg":"trace[1968128832] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7607; }","duration":"310.537829ms","start":"2023-11-05T13:53:27.382Z","end":"2023-11-05T13:53:27.693Z","steps":["trace[1968128832] 'agreement among raft nodes before linearized reading'  (duration: 310.444919ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:53:27.693Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:53:27.382Z","time spent":"310.664321ms","remote":"127.0.0.1:49276","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-11-05T13:54:29.640Z","caller":"traceutil/trace.go:171","msg":"trace[1678479526] transaction","detail":"{read_only:false; response_revision:7662; number_of_response:1; }","duration":"148.960898ms","start":"2023-11-05T13:54:29.490Z","end":"2023-11-05T13:54:29.639Z","steps":["trace[1678479526] 'process raft request'  (duration: 145.473099ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:56:11.834Z","caller":"traceutil/trace.go:171","msg":"trace[526694837] transaction","detail":"{read_only:false; response_revision:7743; number_of_response:1; }","duration":"261.07292ms","start":"2023-11-05T13:56:11.573Z","end":"2023-11-05T13:56:11.834Z","steps":["trace[526694837] 'process raft request'  (duration: 260.960734ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:56:12.184Z","caller":"traceutil/trace.go:171","msg":"trace[1239985845] transaction","detail":"{read_only:false; response_revision:7744; number_of_response:1; }","duration":"152.541822ms","start":"2023-11-05T13:56:12.032Z","end":"2023-11-05T13:56:12.184Z","steps":["trace[1239985845] 'process raft request'  (duration: 152.401074ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T13:56:19.447Z","caller":"traceutil/trace.go:171","msg":"trace[282469348] transaction","detail":"{read_only:false; response_revision:7748; number_of_response:1; }","duration":"985.734058ms","start":"2023-11-05T13:56:18.461Z","end":"2023-11-05T13:56:19.447Z","steps":["trace[282469348] 'process raft request'  (duration: 985.585272ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T13:56:19.523Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T13:56:18.460Z","time spent":"986.50443ms","remote":"127.0.0.1:49144","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:7747 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-05T13:58:00.539Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7584}
{"level":"info","ts":"2023-11-05T13:58:00.744Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7584,"took":"184.111479ms","hash":1298511514}
{"level":"info","ts":"2023-11-05T13:58:00.744Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1298511514,"revision":7584,"compact-revision":7328}
{"level":"warn","ts":"2023-11-05T14:00:10.682Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.168254ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-11-05T14:00:10.683Z","caller":"traceutil/trace.go:171","msg":"trace[843150730] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:7933; }","duration":"121.00936ms","start":"2023-11-05T14:00:10.562Z","end":"2023-11-05T14:00:10.683Z","steps":["trace[843150730] 'range keys from in-memory index tree'  (duration: 113.076119ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:00:57.577Z","caller":"traceutil/trace.go:171","msg":"trace[1587381589] transaction","detail":"{read_only:false; response_revision:7971; number_of_response:1; }","duration":"113.079071ms","start":"2023-11-05T14:00:57.463Z","end":"2023-11-05T14:00:57.577Z","steps":["trace[1587381589] 'process raft request'  (duration: 99.324636ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:03:00.569Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7829}
{"level":"info","ts":"2023-11-05T14:03:00.574Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7829,"took":"5.245227ms","hash":1903683360}
{"level":"info","ts":"2023-11-05T14:03:00.574Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1903683360,"revision":7829,"compact-revision":7584}
{"level":"info","ts":"2023-11-05T14:03:10.443Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-11-05T14:03:10.458Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2023-11-05T14:03:10.459Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2023-11-05T14:08:00.592Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8069}
{"level":"info","ts":"2023-11-05T14:08:00.596Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8069,"took":"3.592144ms","hash":3988736030}
{"level":"info","ts":"2023-11-05T14:08:00.596Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3988736030,"revision":8069,"compact-revision":7829}
{"level":"info","ts":"2023-11-05T14:13:00.602Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8356}
{"level":"info","ts":"2023-11-05T14:13:00.605Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8356,"took":"2.667204ms","hash":3814766528}
{"level":"info","ts":"2023-11-05T14:13:00.605Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3814766528,"revision":8356,"compact-revision":8069}
{"level":"info","ts":"2023-11-05T14:13:30.299Z","caller":"traceutil/trace.go:171","msg":"trace[1158646142] transaction","detail":"{read_only:false; response_revision:8656; number_of_response:1; }","duration":"386.738843ms","start":"2023-11-05T14:13:29.905Z","end":"2023-11-05T14:13:30.291Z","steps":["trace[1158646142] 'process raft request'  (duration: 386.581723ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T14:13:30.397Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T14:13:29.905Z","time spent":"395.135436ms","remote":"127.0.0.1:49144","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:8655 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-05T14:15:38.345Z","caller":"traceutil/trace.go:171","msg":"trace[1448797136] transaction","detail":"{read_only:false; response_revision:8759; number_of_response:1; }","duration":"217.337728ms","start":"2023-11-05T14:15:38.127Z","end":"2023-11-05T14:15:38.344Z","steps":["trace[1448797136] 'process raft request'  (duration: 217.204787ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:17:42.382Z","caller":"traceutil/trace.go:171","msg":"trace[1456358476] transaction","detail":"{read_only:false; response_revision:8857; number_of_response:1; }","duration":"185.259248ms","start":"2023-11-05T14:17:42.196Z","end":"2023-11-05T14:17:42.382Z","steps":["trace[1456358476] 'process raft request'  (duration: 185.141663ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:18:00.609Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8633}
{"level":"info","ts":"2023-11-05T14:18:00.611Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8633,"took":"1.110663ms","hash":3771991933}
{"level":"info","ts":"2023-11-05T14:18:00.611Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3771991933,"revision":8633,"compact-revision":8356}
{"level":"info","ts":"2023-11-05T14:18:39.179Z","caller":"traceutil/trace.go:171","msg":"trace[1125817096] transaction","detail":"{read_only:false; response_revision:8903; number_of_response:1; }","duration":"129.621942ms","start":"2023-11-05T14:18:39.050Z","end":"2023-11-05T14:18:39.179Z","steps":["trace[1125817096] 'process raft request'  (duration: 129.508515ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:20:11.054Z","caller":"traceutil/trace.go:171","msg":"trace[1465085860] transaction","detail":"{read_only:false; response_revision:8976; number_of_response:1; }","duration":"267.12157ms","start":"2023-11-05T14:20:10.786Z","end":"2023-11-05T14:20:11.054Z","steps":["trace[1465085860] 'process raft request'  (duration: 266.994216ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:20:11.065Z","caller":"traceutil/trace.go:171","msg":"trace[909268052] transaction","detail":"{read_only:false; response_revision:8977; number_of_response:1; }","duration":"187.173841ms","start":"2023-11-05T14:20:10.878Z","end":"2023-11-05T14:20:11.065Z","steps":["trace[909268052] 'process raft request'  (duration: 187.049373ms)"],"step_count":1}
{"level":"info","ts":"2023-11-05T14:23:00.614Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8873}
{"level":"info","ts":"2023-11-05T14:23:00.616Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8873,"took":"1.442204ms","hash":3605208611}
{"level":"info","ts":"2023-11-05T14:23:00.616Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3605208611,"revision":8873,"compact-revision":8633}
{"level":"info","ts":"2023-11-05T14:25:34.672Z","caller":"traceutil/trace.go:171","msg":"trace[1053863759] transaction","detail":"{read_only:false; response_revision:9296; number_of_response:1; }","duration":"595.434531ms","start":"2023-11-05T14:25:34.077Z","end":"2023-11-05T14:25:34.672Z","steps":["trace[1053863759] 'process raft request'  (duration: 595.347034ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-05T14:25:35.851Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-05T14:25:34.077Z","time spent":"1.098317661s","remote":"127.0.0.1:49144","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:9295 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-05T14:28:00.620Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9174}
{"level":"info","ts":"2023-11-05T14:28:00.622Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9174,"took":"2.37928ms","hash":533558754}
{"level":"info","ts":"2023-11-05T14:28:00.622Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":533558754,"revision":9174,"compact-revision":8873}

* 
* ==> kernel <==
*  14:30:44 up  1:18,  0 users,  load average: 0.10, 0.49, 0.67
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [310f2eae0f35] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.365551       1 logging.go:59] [core] [Channel #40 SubChannel #41] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.412739       1 logging.go:59] [core] [Channel #13 SubChannel #14] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.463650       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.518863       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.567448       1 logging.go:59] [core] [Channel #4 SubChannel #5] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.600976       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1105 13:33:45.640728       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [f5f3e813f1f0] <==
* I1105 13:38:04.727705       1 apf_controller.go:366] Running API Priority and Fairness config worker
I1105 13:38:04.728237       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I1105 13:38:04.741294       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1105 13:38:05.125851       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1105 13:38:05.204085       1 cache.go:39] Caches are synced for autoregister controller
I1105 13:38:05.210767       1 trace.go:219] Trace[37239200]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/serviceips,type:*core.RangeAllocation,resource:serviceipallocations (05-Nov-2023 13:38:04.551) (total time: 658ms):
Trace[37239200]: ---"initial value restored" 658ms (13:38:05.210)
Trace[37239200]: [658.800557ms] [658.800557ms] END
I1105 13:38:05.413925       1 trace.go:219] Trace[335796924]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b895adcd-e96d-4c40-9541-44d62fc0d755,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (05-Nov-2023 13:38:04.040) (total time: 1373ms):
Trace[335796924]: ["GuaranteedUpdate etcd3" audit-id:b895adcd-e96d-4c40-9541-44d62fc0d755,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1373ms (13:38:04.040)
Trace[335796924]:  ---"About to Encode" 563ms (13:38:04.603)
Trace[335796924]:  ---"Txn call completed" 810ms (13:38:05.413)]
Trace[335796924]: [1.373671311s] [1.373671311s] END
I1105 13:38:05.504342       1 trace.go:219] Trace[1513070336]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f032e533-73c2-4116-b0ee-3e391fac5b7f,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (05-Nov-2023 13:38:03.923) (total time: 1581ms):
Trace[1513070336]: ---"limitedReadBody succeeded" len:2661 18ms (13:38:03.941)
Trace[1513070336]: ---"Write to database call failed" len:2661,err:nodes "minikube" already exists 71ms (13:38:05.503)
Trace[1513070336]: [1.581081275s] [1.581081275s] END
I1105 13:38:05.517191       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1105 13:38:13.313496       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1105 13:38:13.359528       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1105 13:38:14.085925       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1105 13:38:14.185612       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1105 13:38:14.217356       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1105 13:38:20.315638       1 trace.go:219] Trace[1918022357]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7792c615-05d2-4ef8-a59d-210037e6ef29,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PATCH (05-Nov-2023 13:38:18.341) (total time: 1974ms):
Trace[1918022357]: ["GuaranteedUpdate etcd3" audit-id:7792c615-05d2-4ef8-a59d-210037e6ef29,key:/pods/kube-system/kube-apiserver-minikube,type:*core.Pod,resource:pods 1974ms (13:38:18.341)
Trace[1918022357]:  ---"About to Encode" 1909ms (13:38:20.250)
Trace[1918022357]:  ---"Txn call completed" 63ms (13:38:20.314)]
Trace[1918022357]: ---"About to check admission control" 1908ms (13:38:20.249)
Trace[1918022357]: ---"Object stored in database" 64ms (13:38:20.314)
Trace[1918022357]: [1.974528114s] [1.974528114s] END
I1105 13:38:24.451034       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1105 13:38:24.720215       1 controller.go:624] quota admission added evaluator for: endpoints
I1105 13:51:42.848120       1 alloc.go:330] "allocated clusterIPs" service="karsajobs-app/karsajobs-ui-service" clusterIPs=map[IPv4:10.108.157.19]
I1105 13:51:53.562714       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1105 13:52:11.346744       1 trace.go:219] Trace[28807712]: "Update" accept:application/json, */*,audit-id:93956405-e3da-43a2-8da2-b6c78f5bc824,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (05-Nov-2023 13:52:10.774) (total time: 571ms):
Trace[28807712]: ["GuaranteedUpdate etcd3" audit-id:93956405-e3da-43a2-8da2-b6c78f5bc824,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 571ms (13:52:10.775)
Trace[28807712]:  ---"Txn call completed" 570ms (13:52:11.346)]
Trace[28807712]: [571.547231ms] [571.547231ms] END
I1105 13:53:27.687766       1 trace.go:219] Trace[797384184]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:aaa9d529-f4d1-48fa-971a-20b8252cef74,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (05-Nov-2023 13:53:25.846) (total time: 1840ms):
Trace[797384184]: ["GuaranteedUpdate etcd3" audit-id:aaa9d529-f4d1-48fa-971a-20b8252cef74,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1840ms (13:53:25.847)
Trace[797384184]:  ---"Txn call completed" 1839ms (13:53:27.687)]
Trace[797384184]: [1.840848582s] [1.840848582s] END
I1105 13:53:27.696698       1 trace.go:219] Trace[443415633]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4fcdf162-60b7-4d97-8d14-9850708584fe,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PATCH (05-Nov-2023 13:53:26.407) (total time: 1288ms):
Trace[443415633]: ["GuaranteedUpdate etcd3" audit-id:4fcdf162-60b7-4d97-8d14-9850708584fe,key:/minions/minikube,type:*core.Node,resource:nodes 1288ms (13:53:26.407)
Trace[443415633]:  ---"Txn call completed" 1284ms (13:53:27.694)]
Trace[443415633]: ---"Object stored in database" 1284ms (13:53:27.694)
Trace[443415633]: [1.288995808s] [1.288995808s] END
I1105 13:53:27.697738       1 trace.go:219] Trace[35189343]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ccfa59aa-a3ae-44e8-bc02-31612868babc,client:192.168.49.2,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:resource,url:/api/v1/namespaces/karsajobs-app/persistentvolumeclaims/mongo-pvc,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (05-Nov-2023 13:53:27.152) (total time: 545ms):
Trace[35189343]: ---"About to write a response" 544ms (13:53:27.697)
Trace[35189343]: [545.031152ms] [545.031152ms] END
I1105 13:56:19.524579       1 trace.go:219] Trace[878814106]: "Update" accept:application/json, */*,audit-id:a02748d0-71d4-413a-97b2-79467278b8f7,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (05-Nov-2023 13:56:18.459) (total time: 1065ms):
Trace[878814106]: ["GuaranteedUpdate etcd3" audit-id:a02748d0-71d4-413a-97b2-79467278b8f7,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1064ms (13:56:18.459)
Trace[878814106]:  ---"Txn call completed" 1064ms (13:56:19.524)]
Trace[878814106]: [1.065001066s] [1.065001066s] END
I1105 14:09:37.687058       1 alloc.go:330] "allocated clusterIPs" service="karsajobs-app/karsajobs-ui" clusterIPs=map[IPv4:10.111.160.48]
I1105 14:22:07.629193       1 alloc.go:330] "allocated clusterIPs" service="karsajobs-app/karsajobs-ui" clusterIPs=map[IPv4:10.96.76.203]
I1105 14:25:35.905906       1 trace.go:219] Trace[385563581]: "Update" accept:application/json, */*,audit-id:47481cb5-7bcb-4519-999f-21e966c27c02,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (05-Nov-2023 14:25:34.076) (total time: 1800ms):
Trace[385563581]: ["GuaranteedUpdate etcd3" audit-id:47481cb5-7bcb-4519-999f-21e966c27c02,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1800ms (14:25:34.076)
Trace[385563581]:  ---"Txn call completed" 1799ms (14:25:35.876)]
Trace[385563581]: [1.800246942s] [1.800246942s] END

* 
* ==> kube-controller-manager [468f2f7cd9a4] <==
* I1105 13:38:23.932149       1 shared_informer.go:318] Caches are synced for node
I1105 13:38:23.932246       1 range_allocator.go:174] "Sending events to api server"
I1105 13:38:23.932271       1 range_allocator.go:178] "Starting range CIDR allocator"
I1105 13:38:23.932275       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1105 13:38:23.932280       1 shared_informer.go:318] Caches are synced for cidrallocator
I1105 13:38:23.937976       1 shared_informer.go:318] Caches are synced for TTL after finished
I1105 13:38:23.948195       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1105 13:38:23.954314       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1105 13:38:23.966622       1 shared_informer.go:318] Caches are synced for namespace
I1105 13:38:23.966846       1 shared_informer.go:318] Caches are synced for service account
I1105 13:38:23.966919       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1105 13:38:23.966943       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1105 13:38:23.970636       1 shared_informer.go:318] Caches are synced for expand
I1105 13:38:24.006985       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1105 13:38:24.016753       1 shared_informer.go:318] Caches are synced for GC
I1105 13:38:24.020073       1 shared_informer.go:318] Caches are synced for cronjob
I1105 13:38:24.027618       1 shared_informer.go:318] Caches are synced for persistent volume
I1105 13:38:24.027930       1 shared_informer.go:318] Caches are synced for stateful set
I1105 13:38:24.032011       1 shared_informer.go:318] Caches are synced for HPA
I1105 13:38:24.032193       1 shared_informer.go:318] Caches are synced for crt configmap
I1105 13:38:24.032264       1 shared_informer.go:318] Caches are synced for PVC protection
I1105 13:38:24.036714       1 shared_informer.go:318] Caches are synced for taint
I1105 13:38:24.037192       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1105 13:38:24.046273       1 shared_informer.go:318] Caches are synced for ReplicationController
I1105 13:38:24.046759       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1105 13:38:24.050087       1 taint_manager.go:211] "Sending events to api server"
I1105 13:38:24.112571       1 shared_informer.go:318] Caches are synced for deployment
I1105 13:38:24.117292       1 shared_informer.go:318] Caches are synced for daemon sets
I1105 13:38:24.128008       1 shared_informer.go:318] Caches are synced for attach detach
I1105 13:38:24.130841       1 shared_informer.go:318] Caches are synced for job
I1105 13:38:24.136772       1 shared_informer.go:318] Caches are synced for ephemeral
I1105 13:38:24.149358       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1105 13:38:24.150119       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1105 13:38:24.150355       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1105 13:38:24.151237       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1105 13:38:24.151379       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1105 13:38:24.205310       1 shared_informer.go:318] Caches are synced for disruption
I1105 13:38:24.206034       1 shared_informer.go:318] Caches are synced for resource quota
I1105 13:38:24.206206       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1105 13:38:24.212717       1 shared_informer.go:318] Caches are synced for resource quota
I1105 13:38:24.228750       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1105 13:38:24.236899       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1105 13:38:24.303889       1 shared_informer.go:318] Caches are synced for endpoint
I1105 13:38:24.334629       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1105 13:38:24.611933       1 shared_informer.go:318] Caches are synced for garbage collector
I1105 13:38:24.706004       1 shared_informer.go:318] Caches are synced for garbage collector
I1105 13:38:24.706037       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1105 13:51:53.575982       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-ui-deployment-7d6c6867b5 to 1"
I1105 13:51:53.616440       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-deployment-7d6c6867b5-xjbgw"
I1105 14:06:33.546725       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-deployment-7d6c6867b5-64whw"
I1105 14:07:17.340261       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-deployment-7d6c6867b5-9bf7r"
I1105 14:09:01.951012       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set karsajobs-ui-deployment-7d6c6867b5 to 0 from 1"
I1105 14:09:01.962691       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: karsajobs-ui-deployment-7d6c6867b5-9bf7r"
I1105 14:09:46.232038       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-ui-7d6c6867b5 to 1"
I1105 14:09:46.268518       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-7d6c6867b5-mdcxz"
I1105 14:21:21.887897       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-7d6c6867b5-5v6bq"
I1105 14:22:13.148322       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-ui-6c64569b8f to 1"
I1105 14:22:13.169268       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-6c64569b8f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-6c64569b8f-nnr74"
I1105 14:22:17.692712       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set karsajobs-ui-7d6c6867b5 to 0 from 1"
I1105 14:22:17.719358       1 event.go:307] "Event occurred" object="karsajobs-app/karsajobs-ui-7d6c6867b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: karsajobs-ui-7d6c6867b5-5v6bq"

* 
* ==> kube-controller-manager [f2081a31d0a6] <==
* I1105 13:21:19.615002       1 shared_informer.go:311] Waiting for caches to sync for HPA
E1105 13:21:19.626388       1 core.go:92] "Failed to start service controller" err="WARNING: no cloud provider provided, services of type LoadBalancer will fail"
I1105 13:21:19.626475       1 controllermanager.go:616] "Warning: skipping controller" controller="service"
I1105 13:21:19.630447       1 controllermanager.go:638] "Started controller" controller="persistentvolume-binder"
I1105 13:21:19.630536       1 pv_controller_base.go:323] "Starting persistent volume controller"
I1105 13:21:19.630549       1 shared_informer.go:311] Waiting for caches to sync for persistent volume
I1105 13:21:19.642955       1 controllermanager.go:638] "Started controller" controller="ephemeral-volume"
I1105 13:21:19.643105       1 controller.go:169] "Starting ephemeral volume controller"
I1105 13:21:19.643119       1 shared_informer.go:311] Waiting for caches to sync for ephemeral
I1105 13:21:19.661557       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1105 13:21:19.751394       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1105 13:21:19.803584       1 shared_informer.go:318] Caches are synced for service account
I1105 13:21:19.805661       1 shared_informer.go:318] Caches are synced for node
I1105 13:21:19.817177       1 range_allocator.go:174] "Sending events to api server"
I1105 13:21:19.818909       1 shared_informer.go:318] Caches are synced for namespace
I1105 13:21:19.819045       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1105 13:21:19.850355       1 range_allocator.go:178] "Starting range CIDR allocator"
I1105 13:21:19.850377       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1105 13:21:19.850384       1 shared_informer.go:318] Caches are synced for cidrallocator
I1105 13:21:19.850848       1 shared_informer.go:318] Caches are synced for crt configmap
I1105 13:21:19.851493       1 shared_informer.go:318] Caches are synced for TTL
I1105 13:21:19.851739       1 shared_informer.go:318] Caches are synced for taint
I1105 13:21:19.851798       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1105 13:21:19.851879       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1105 13:21:19.851918       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1105 13:21:19.851937       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1105 13:21:19.851979       1 taint_manager.go:211] "Sending events to api server"
I1105 13:21:19.852218       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1105 13:21:19.852235       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1105 13:21:19.853303       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1105 13:21:19.853763       1 shared_informer.go:318] Caches are synced for PV protection
I1105 13:21:19.872356       1 shared_informer.go:318] Caches are synced for endpoint
I1105 13:21:19.872413       1 shared_informer.go:318] Caches are synced for GC
I1105 13:21:19.873161       1 shared_informer.go:318] Caches are synced for expand
I1105 13:21:19.876538       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1105 13:21:19.877683       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1105 13:21:19.878208       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1105 13:21:19.878710       1 shared_informer.go:318] Caches are synced for deployment
I1105 13:21:19.881007       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1105 13:21:19.888507       1 shared_informer.go:318] Caches are synced for ReplicationController
I1105 13:21:19.901597       1 shared_informer.go:318] Caches are synced for cronjob
I1105 13:21:19.915917       1 shared_informer.go:318] Caches are synced for job
I1105 13:21:19.881026       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1105 13:21:19.881032       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1105 13:21:19.917537       1 shared_informer.go:318] Caches are synced for disruption
I1105 13:21:19.917574       1 shared_informer.go:318] Caches are synced for HPA
I1105 13:21:19.917592       1 shared_informer.go:318] Caches are synced for stateful set
I1105 13:21:19.924901       1 shared_informer.go:318] Caches are synced for daemon sets
I1105 13:21:19.924940       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1105 13:21:19.925408       1 shared_informer.go:318] Caches are synced for attach detach
I1105 13:21:19.926574       1 shared_informer.go:318] Caches are synced for PVC protection
I1105 13:21:19.931207       1 shared_informer.go:318] Caches are synced for persistent volume
I1105 13:21:19.945064       1 shared_informer.go:318] Caches are synced for ephemeral
I1105 13:21:19.966004       1 shared_informer.go:318] Caches are synced for resource quota
I1105 13:21:19.966365       1 shared_informer.go:318] Caches are synced for TTL after finished
I1105 13:21:19.997982       1 shared_informer.go:318] Caches are synced for resource quota
I1105 13:21:19.998037       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1105 13:21:20.274592       1 shared_informer.go:318] Caches are synced for garbage collector
I1105 13:21:20.274886       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1105 13:21:20.288489       1 shared_informer.go:318] Caches are synced for garbage collector

* 
* ==> kube-proxy [1385ee9e0583] <==
* I1105 13:38:15.650863       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1105 13:38:15.651625       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1105 13:38:15.651795       1 server_others.go:554] "Using iptables proxy"
I1105 13:38:16.213926       1 server_others.go:192] "Using iptables Proxier"
I1105 13:38:16.216266       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1105 13:38:16.216359       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1105 13:38:16.216379       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1105 13:38:16.221835       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1105 13:38:16.252823       1 server.go:658] "Version info" version="v1.27.4"
I1105 13:38:16.252881       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 13:38:16.338269       1 config.go:188] "Starting service config controller"
I1105 13:38:16.340120       1 shared_informer.go:311] Waiting for caches to sync for service config
I1105 13:38:16.341420       1 config.go:315] "Starting node config controller"
I1105 13:38:16.341624       1 shared_informer.go:311] Waiting for caches to sync for node config
I1105 13:38:16.352872       1 config.go:97] "Starting endpoint slice config controller"
I1105 13:38:16.352914       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1105 13:38:16.441968       1 shared_informer.go:318] Caches are synced for node config
I1105 13:38:16.442050       1 shared_informer.go:318] Caches are synced for service config
I1105 13:38:16.453301       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [2e205cd7fc79] <==
* I1105 13:21:13.951005       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1105 13:21:13.951099       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1105 13:21:13.951134       1 server_others.go:554] "Using iptables proxy"
I1105 13:21:14.399653       1 server_others.go:192] "Using iptables Proxier"
I1105 13:21:14.399943       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1105 13:21:14.400151       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1105 13:21:14.400247       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1105 13:21:14.403052       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1105 13:21:14.415464       1 server.go:658] "Version info" version="v1.27.4"
I1105 13:21:14.415949       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 13:21:14.447190       1 config.go:97] "Starting endpoint slice config controller"
I1105 13:21:14.464431       1 config.go:188] "Starting service config controller"
I1105 13:21:14.464536       1 shared_informer.go:311] Waiting for caches to sync for service config
I1105 13:21:14.468582       1 config.go:315] "Starting node config controller"
I1105 13:21:14.468597       1 shared_informer.go:311] Waiting for caches to sync for node config
I1105 13:21:14.468705       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1105 13:21:14.571395       1 shared_informer.go:318] Caches are synced for node config
I1105 13:21:14.574924       1 shared_informer.go:318] Caches are synced for service config
I1105 13:21:14.574973       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [9548fde39152] <==
* I1105 13:37:59.411220       1 serving.go:348] Generated self-signed cert in-memory
W1105 13:38:04.317829       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1105 13:38:04.318073       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1105 13:38:04.318090       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1105 13:38:04.318098       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1105 13:38:05.512182       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1105 13:38:05.512244       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 13:38:05.631326       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1105 13:38:05.636615       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1105 13:38:05.637121       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1105 13:38:05.718352       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1105 13:38:05.828811       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [ca393e85decb] <==
* W1105 13:20:59.212321       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212348       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212424       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212451       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212516       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212548       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212613       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212649       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212819       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212897       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212829       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.212994       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.213004       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213110       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.213108       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.213186       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213220       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213203       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.212927       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213244       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.213430       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213644       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:20:59.213592       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1105 13:20:59.213721       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1105 13:21:04.169658       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1105 13:21:04.170066       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1105 13:21:04.269213       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1105 13:21:04.170108       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1105 13:21:04.269235       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1105 13:21:04.269138       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1105 13:21:04.269251       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1105 13:21:04.269268       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1105 13:21:04.269275       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1105 13:21:04.269287       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1105 13:21:04.269477       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1105 13:21:04.269489       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1105 13:21:04.269541       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1105 13:21:04.269552       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1105 13:21:04.269611       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1105 13:21:04.269624       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1105 13:21:04.269696       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1105 13:21:04.269709       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1105 13:21:04.269765       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1105 13:21:04.269775       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1105 13:21:04.269823       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1105 13:21:04.269834       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1105 13:21:04.269883       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1105 13:21:04.269896       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1105 13:21:04.269970       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1105 13:21:04.269983       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1105 13:21:04.270041       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1105 13:21:04.270056       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1105 13:21:04.349024       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1105 13:21:04.349057       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1105 13:21:07.601007       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1105 13:33:35.549267       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1105 13:33:35.551033       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1105 13:33:35.551729       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1105 13:33:35.612951       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1105 13:33:35.614079       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Nov 05 14:06:36 minikube kubelet[1586]: I1105 14:06:36.805955    1586 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5-64whw" podStartSLOduration=2.084885289 podCreationTimestamp="2023-11-05 14:06:33 +0000 UTC" firstStartedPulling="2023-11-05 14:06:34.782116272 +0000 UTC m=+1719.011310449" lastFinishedPulling="2023-11-05 14:06:36.503144339 +0000 UTC m=+1720.732338516" observedRunningTime="2023-11-05 14:06:36.805746874 +0000 UTC m=+1721.034941052" watchObservedRunningTime="2023-11-05 14:06:36.805913356 +0000 UTC m=+1721.035107533"
Nov 05 14:07:17 minikube kubelet[1586]: I1105 14:07:17.353200    1586 topology_manager.go:212] "Topology Admit Handler"
Nov 05 14:07:17 minikube kubelet[1586]: E1105 14:07:17.353728    1586 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f7005427-151a-4a45-88a3-b720527b1154" containerName="karsajobs-ui"
Nov 05 14:07:17 minikube kubelet[1586]: I1105 14:07:17.354217    1586 memory_manager.go:346] "RemoveStaleState removing state" podUID="f7005427-151a-4a45-88a3-b720527b1154" containerName="karsajobs-ui"
Nov 05 14:07:17 minikube kubelet[1586]: I1105 14:07:17.467788    1586 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lg9rl\" (UniqueName: \"kubernetes.io/projected/4d274f93-5aa2-4c44-828c-3a9d1861f020-kube-api-access-lg9rl\") pod \"karsajobs-ui-deployment-7d6c6867b5-9bf7r\" (UID: \"4d274f93-5aa2-4c44-828c-3a9d1861f020\") " pod="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5-9bf7r"
Nov 05 14:07:17 minikube kubelet[1586]: I1105 14:07:17.972104    1586 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vrv8h\" (UniqueName: \"kubernetes.io/projected/c0fb908b-05c4-4921-92a2-eaf551175243-kube-api-access-vrv8h\") pod \"c0fb908b-05c4-4921-92a2-eaf551175243\" (UID: \"c0fb908b-05c4-4921-92a2-eaf551175243\") "
Nov 05 14:07:17 minikube kubelet[1586]: I1105 14:07:17.973797    1586 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c0fb908b-05c4-4921-92a2-eaf551175243-kube-api-access-vrv8h" (OuterVolumeSpecName: "kube-api-access-vrv8h") pod "c0fb908b-05c4-4921-92a2-eaf551175243" (UID: "c0fb908b-05c4-4921-92a2-eaf551175243"). InnerVolumeSpecName "kube-api-access-vrv8h". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 05 14:07:18 minikube kubelet[1586]: I1105 14:07:18.073122    1586 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-vrv8h\" (UniqueName: \"kubernetes.io/projected/c0fb908b-05c4-4921-92a2-eaf551175243-kube-api-access-vrv8h\") on node \"minikube\" DevicePath \"\""
Nov 05 14:07:18 minikube kubelet[1586]: I1105 14:07:18.400823    1586 scope.go:115] "RemoveContainer" containerID="27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c"
Nov 05 14:07:18 minikube kubelet[1586]: I1105 14:07:18.447009    1586 scope.go:115] "RemoveContainer" containerID="27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c"
Nov 05 14:07:18 minikube kubelet[1586]: E1105 14:07:18.489189    1586 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c" containerID="27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c"
Nov 05 14:07:18 minikube kubelet[1586]: I1105 14:07:18.489294    1586 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c} err="failed to get container status \"27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c\": rpc error: code = Unknown desc = Error response from daemon: No such container: 27e653f59377f1f494e803822cdfb31817c1f91077fd54df7683420ff058548c"
Nov 05 14:07:20 minikube kubelet[1586]: I1105 14:07:20.137811    1586 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=c0fb908b-05c4-4921-92a2-eaf551175243 path="/var/lib/kubelet/pods/c0fb908b-05c4-4921-92a2-eaf551175243/volumes"
Nov 05 14:07:56 minikube kubelet[1586]: W1105 14:07:56.221177    1586 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 05 14:09:01 minikube kubelet[1586]: I1105 14:09:01.962449    1586 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="karsajobs-app/karsajobs-ui-deployment-7d6c6867b5-9bf7r" podStartSLOduration=102.734600134 podCreationTimestamp="2023-11-05 14:07:17 +0000 UTC" firstStartedPulling="2023-11-05 14:07:18.016005099 +0000 UTC m=+1762.244179942" lastFinishedPulling="2023-11-05 14:07:20.243760442 +0000 UTC m=+1764.471935285" observedRunningTime="2023-11-05 14:07:20.487177294 +0000 UTC m=+1764.715352137" watchObservedRunningTime="2023-11-05 14:09:01.962355477 +0000 UTC m=+1866.189576869"
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.502070    1586 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lg9rl\" (UniqueName: \"kubernetes.io/projected/4d274f93-5aa2-4c44-828c-3a9d1861f020-kube-api-access-lg9rl\") pod \"4d274f93-5aa2-4c44-828c-3a9d1861f020\" (UID: \"4d274f93-5aa2-4c44-828c-3a9d1861f020\") "
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.510517    1586 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4d274f93-5aa2-4c44-828c-3a9d1861f020-kube-api-access-lg9rl" (OuterVolumeSpecName: "kube-api-access-lg9rl") pod "4d274f93-5aa2-4c44-828c-3a9d1861f020" (UID: "4d274f93-5aa2-4c44-828c-3a9d1861f020"). InnerVolumeSpecName "kube-api-access-lg9rl". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.603321    1586 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-lg9rl\" (UniqueName: \"kubernetes.io/projected/4d274f93-5aa2-4c44-828c-3a9d1861f020-kube-api-access-lg9rl\") on node \"minikube\" DevicePath \"\""
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.880120    1586 scope.go:115] "RemoveContainer" containerID="07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9"
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.941468    1586 scope.go:115] "RemoveContainer" containerID="07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9"
Nov 05 14:09:02 minikube kubelet[1586]: E1105 14:09:02.943374    1586 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9" containerID="07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9"
Nov 05 14:09:02 minikube kubelet[1586]: I1105 14:09:02.943490    1586 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9} err="failed to get container status \"07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9\": rpc error: code = Unknown desc = Error response from daemon: No such container: 07357f2a48399fcecdf6d382fefaf00bb2642c032bfa02f85c762b6e72a45ea9"
Nov 05 14:09:04 minikube kubelet[1586]: I1105 14:09:04.139521    1586 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=4d274f93-5aa2-4c44-828c-3a9d1861f020 path="/var/lib/kubelet/pods/4d274f93-5aa2-4c44-828c-3a9d1861f020/volumes"
Nov 05 14:09:46 minikube kubelet[1586]: I1105 14:09:46.318279    1586 topology_manager.go:212] "Topology Admit Handler"
Nov 05 14:09:46 minikube kubelet[1586]: E1105 14:09:46.323959    1586 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c0fb908b-05c4-4921-92a2-eaf551175243" containerName="karsajobs-ui"
Nov 05 14:09:46 minikube kubelet[1586]: E1105 14:09:46.324080    1586 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4d274f93-5aa2-4c44-828c-3a9d1861f020" containerName="karsajobs-ui"
Nov 05 14:09:46 minikube kubelet[1586]: I1105 14:09:46.324146    1586 memory_manager.go:346] "RemoveStaleState removing state" podUID="c0fb908b-05c4-4921-92a2-eaf551175243" containerName="karsajobs-ui"
Nov 05 14:09:46 minikube kubelet[1586]: I1105 14:09:46.324157    1586 memory_manager.go:346] "RemoveStaleState removing state" podUID="4d274f93-5aa2-4c44-828c-3a9d1861f020" containerName="karsajobs-ui"
Nov 05 14:09:46 minikube kubelet[1586]: I1105 14:09:46.383036    1586 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f2sgc\" (UniqueName: \"kubernetes.io/projected/8c9492cb-cef7-44e7-8fdd-a98f83c37160-kube-api-access-f2sgc\") pod \"karsajobs-ui-7d6c6867b5-mdcxz\" (UID: \"8c9492cb-cef7-44e7-8fdd-a98f83c37160\") " pod="karsajobs-app/karsajobs-ui-7d6c6867b5-mdcxz"
Nov 05 14:09:49 minikube kubelet[1586]: I1105 14:09:49.578980    1586 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="karsajobs-app/karsajobs-ui-7d6c6867b5-mdcxz" podStartSLOduration=1.707064224 podCreationTimestamp="2023-11-05 14:09:46 +0000 UTC" firstStartedPulling="2023-11-05 14:09:47.159630052 +0000 UTC m=+1911.387571806" lastFinishedPulling="2023-11-05 14:09:49.031371649 +0000 UTC m=+1913.259313393" observedRunningTime="2023-11-05 14:09:49.578622276 +0000 UTC m=+1913.806564030" watchObservedRunningTime="2023-11-05 14:09:49.578805811 +0000 UTC m=+1913.806747565"
Nov 05 14:12:56 minikube kubelet[1586]: W1105 14:12:56.218892    1586 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 05 14:17:56 minikube kubelet[1586]: W1105 14:17:56.219233    1586 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 05 14:21:21 minikube kubelet[1586]: I1105 14:21:21.913816    1586 topology_manager.go:212] "Topology Admit Handler"
Nov 05 14:21:22 minikube kubelet[1586]: I1105 14:21:22.087621    1586 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f58th\" (UniqueName: \"kubernetes.io/projected/59aa7220-89b4-4216-9d09-bd41aaf9988d-kube-api-access-f58th\") pod \"karsajobs-ui-7d6c6867b5-5v6bq\" (UID: \"59aa7220-89b4-4216-9d09-bd41aaf9988d\") " pod="karsajobs-app/karsajobs-ui-7d6c6867b5-5v6bq"
Nov 05 14:21:22 minikube kubelet[1586]: I1105 14:21:22.948379    1586 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="501b55f9e9c5350101e61e3dcc49421b733f7230a0e6611ada61e01440fa40af"
Nov 05 14:21:23 minikube kubelet[1586]: I1105 14:21:23.803986    1586 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-f2sgc\" (UniqueName: \"kubernetes.io/projected/8c9492cb-cef7-44e7-8fdd-a98f83c37160-kube-api-access-f2sgc\") pod \"8c9492cb-cef7-44e7-8fdd-a98f83c37160\" (UID: \"8c9492cb-cef7-44e7-8fdd-a98f83c37160\") "
Nov 05 14:21:23 minikube kubelet[1586]: I1105 14:21:23.807313    1586 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8c9492cb-cef7-44e7-8fdd-a98f83c37160-kube-api-access-f2sgc" (OuterVolumeSpecName: "kube-api-access-f2sgc") pod "8c9492cb-cef7-44e7-8fdd-a98f83c37160" (UID: "8c9492cb-cef7-44e7-8fdd-a98f83c37160"). InnerVolumeSpecName "kube-api-access-f2sgc". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 05 14:21:23 minikube kubelet[1586]: I1105 14:21:23.904658    1586 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-f2sgc\" (UniqueName: \"kubernetes.io/projected/8c9492cb-cef7-44e7-8fdd-a98f83c37160-kube-api-access-f2sgc\") on node \"minikube\" DevicePath \"\""
Nov 05 14:21:23 minikube kubelet[1586]: I1105 14:21:23.980305    1586 scope.go:115] "RemoveContainer" containerID="4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea"
Nov 05 14:21:24 minikube kubelet[1586]: I1105 14:21:24.037245    1586 scope.go:115] "RemoveContainer" containerID="4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea"
Nov 05 14:21:24 minikube kubelet[1586]: E1105 14:21:24.040970    1586 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea" containerID="4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea"
Nov 05 14:21:24 minikube kubelet[1586]: I1105 14:21:24.041091    1586 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea} err="failed to get container status \"4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4aba49048e428b3c4bf91880e981e6978643e49e4ad1f9926c0ea0cac734bdea"
Nov 05 14:21:24 minikube kubelet[1586]: I1105 14:21:24.134776    1586 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=8c9492cb-cef7-44e7-8fdd-a98f83c37160 path="/var/lib/kubelet/pods/8c9492cb-cef7-44e7-8fdd-a98f83c37160/volumes"
Nov 05 14:22:13 minikube kubelet[1586]: I1105 14:22:13.195201    1586 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="karsajobs-app/karsajobs-ui-7d6c6867b5-5v6bq" podStartSLOduration=50.078318338 podCreationTimestamp="2023-11-05 14:21:21 +0000 UTC" firstStartedPulling="2023-11-05 14:21:23.030540473 +0000 UTC m=+2607.262055724" lastFinishedPulling="2023-11-05 14:21:25.140526975 +0000 UTC m=+2609.372042226" observedRunningTime="2023-11-05 14:21:26.059687307 +0000 UTC m=+2610.291713231" watchObservedRunningTime="2023-11-05 14:22:13.18830484 +0000 UTC m=+2657.420646175"
Nov 05 14:22:13 minikube kubelet[1586]: I1105 14:22:13.196082    1586 topology_manager.go:212] "Topology Admit Handler"
Nov 05 14:22:13 minikube kubelet[1586]: E1105 14:22:13.197774    1586 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8c9492cb-cef7-44e7-8fdd-a98f83c37160" containerName="karsajobs-ui"
Nov 05 14:22:13 minikube kubelet[1586]: I1105 14:22:13.205301    1586 memory_manager.go:346] "RemoveStaleState removing state" podUID="8c9492cb-cef7-44e7-8fdd-a98f83c37160" containerName="karsajobs-ui"
Nov 05 14:22:13 minikube kubelet[1586]: I1105 14:22:13.342268    1586 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nrs4w\" (UniqueName: \"kubernetes.io/projected/ec18fba0-64e4-4174-a956-e393176fe8cf-kube-api-access-nrs4w\") pod \"karsajobs-ui-6c64569b8f-nnr74\" (UID: \"ec18fba0-64e4-4174-a956-e393176fe8cf\") " pod="karsajobs-app/karsajobs-ui-6c64569b8f-nnr74"
Nov 05 14:22:14 minikube kubelet[1586]: I1105 14:22:14.000529    1586 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7cba0655d7104b892484c2496e89a91e870f7189232c3cf5874df88e604375ef"
Nov 05 14:22:17 minikube kubelet[1586]: I1105 14:22:17.711285    1586 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="karsajobs-app/karsajobs-ui-6c64569b8f-nnr74" podStartSLOduration=2.909708493 podCreationTimestamp="2023-11-05 14:22:13 +0000 UTC" firstStartedPulling="2023-11-05 14:22:14.05363858 +0000 UTC m=+2658.285979915" lastFinishedPulling="2023-11-05 14:22:15.855174221 +0000 UTC m=+2660.087515566" observedRunningTime="2023-11-05 14:22:17.62835252 +0000 UTC m=+2661.860693875" watchObservedRunningTime="2023-11-05 14:22:17.711244144 +0000 UTC m=+2661.943585479"
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.595756    1586 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-f58th\" (UniqueName: \"kubernetes.io/projected/59aa7220-89b4-4216-9d09-bd41aaf9988d-kube-api-access-f58th\") pod \"59aa7220-89b4-4216-9d09-bd41aaf9988d\" (UID: \"59aa7220-89b4-4216-9d09-bd41aaf9988d\") "
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.599045    1586 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/59aa7220-89b4-4216-9d09-bd41aaf9988d-kube-api-access-f58th" (OuterVolumeSpecName: "kube-api-access-f58th") pod "59aa7220-89b4-4216-9d09-bd41aaf9988d" (UID: "59aa7220-89b4-4216-9d09-bd41aaf9988d"). InnerVolumeSpecName "kube-api-access-f58th". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.649960    1586 scope.go:115] "RemoveContainer" containerID="4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c"
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.696337    1586 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-f58th\" (UniqueName: \"kubernetes.io/projected/59aa7220-89b4-4216-9d09-bd41aaf9988d-kube-api-access-f58th\") on node \"minikube\" DevicePath \"\""
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.758450    1586 scope.go:115] "RemoveContainer" containerID="4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c"
Nov 05 14:22:18 minikube kubelet[1586]: E1105 14:22:18.759888    1586 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c" containerID="4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c"
Nov 05 14:22:18 minikube kubelet[1586]: I1105 14:22:18.759935    1586 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c} err="failed to get container status \"4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4f74f9757b4d28db7b7cb8732bb3400536d7bcaf0793ca5bc4e5c89d2520d15c"
Nov 05 14:22:20 minikube kubelet[1586]: I1105 14:22:20.174923    1586 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=59aa7220-89b4-4216-9d09-bd41aaf9988d path="/var/lib/kubelet/pods/59aa7220-89b4-4216-9d09-bd41aaf9988d/volumes"
Nov 05 14:22:56 minikube kubelet[1586]: W1105 14:22:56.216584    1586 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 05 14:27:56 minikube kubelet[1586]: W1105 14:27:56.218282    1586 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [7121924e9151] <==
* I1105 13:38:13.993225       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1105 13:38:24.077289       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [90b7e062a301] <==
* I1105 13:38:42.589493       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1105 13:38:42.670942       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1105 13:38:42.675194       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1105 13:39:00.153991       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1105 13:39:00.156327       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_46f903d7-26b0-49f2-adb3-c4bcf5c9cb9c!
I1105 13:39:00.157322       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d433e004-f560-4d80-985d-518f74f1bea6", APIVersion:"v1", ResourceVersion:"6900", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_46f903d7-26b0-49f2-adb3-c4bcf5c9cb9c became leader
I1105 13:39:00.259561       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_46f903d7-26b0-49f2-adb3-c4bcf5c9cb9c!

